# ğŸ“˜ Introduction to Natural Language Processing (NLP)

## Simple English Notes

---

## 1ï¸âƒ£ What is NLP?

**Natural Language Processing (NLP)** is a field of study that helps computers **understand, process, and generate human language**.

It is a combination of:

* **Linguistics** â†’ understanding language rules
* **Computer Science** â†’ programming & algorithms
* **Artificial Intelligence (AI)** â†’ making machines intelligent

### ğŸ”‘ Main Goal of NLP

To make machines:

* Understand what humans say or write
* Respond back in **natural human language**

NLP is not only about understanding text, but also about **speaking, replying, and interacting naturally**.

---

## 2ï¸âƒ£ Why is NLP Important? (Need of NLP)

Humans became powerful mainly because of:

* Communication
* Language

We shared ideas, taught future generations, and improved over time.

### ğŸš€ Next Big Revolution

The next big step in human progress is:

> **Talking to machines the same way we talk to humans**

### Evolution of Interaction

* **Earlier**:

  * Machines â†’ buttons, switches
  * Computers â†’ programming languages
  * Smartphones â†’ touch

* **Now**:

  * Voice
  * Text
  * Conversation

### Examples

* Google Assistant
* Siri
* Alexa
* Chatbots

ğŸ‘‰ NLP makes machines **friendly, usable, and intelligent** for everyone.

---

## 3ï¸âƒ£ Real-World Applications of NLP

### ğŸŸ¢ Everyday Applications

* Chatbots (customer support)
* Voice assistants
* Google Search smart answers
* Auto-reply emails
* Spam email detection

### ğŸŸ¢ Business & Industry

* Targeted advertisements
* Product review analysis
* Customer feedback analysis

### ğŸŸ¢ Social Media

* Hate-speech detection
* Adult content filtering
* Sentiment analysis on tweets
* Trend & opinion mining

### ğŸŸ¢ Search Engines

* Direct answers (no need to open websites)
* Knowledge graphs
* Smart suggestions

---

## 4ï¸âƒ£ Common NLP Tasks (Very Important)

If you want to become an **NLP Engineer**, these tasks are core skills:

### 1ï¸âƒ£ Text / Document Classification

Classifying text into categories:

* Sports
* Politics
* Technology
* Spam / Not spam

---

### 2ï¸âƒ£ Sentiment Analysis

Finding emotions from text:

* Positive
* Negative
* Neutral

Used in:

* Product reviews
* Movie reviews
* Social media

---

### 3ï¸âƒ£ Information Extraction

Extracting useful information:

* Names
* Locations
* Dates
* Product names

---

### 4ï¸âƒ£ Parts of Speech (POS) Tagging

Assigning labels to words:

* Noun
* Verb
* Adjective

Helps machines understand **sentence structure**.

---

### 5ï¸âƒ£ Language Detection & Translation

* Detect language automatically
* Translate text (Google Translate)

---

### 6ï¸âƒ£ Speech to Text & Text to Speech

* Voice â†’ Text
* Text â†’ Voice

Used in:

* Voice assistants
* Accessibility tools

---

### 7ï¸âƒ£ Text Summarization

* Convert long text into short summary
* Example: News apps like **Inshorts**

---

### 8ï¸âƒ£ Topic Modeling

Finding main topics in large text data

**Example**:

* Cricket article â†’ IPL, players, matches

---

### 9ï¸âƒ£ Text Generation

* Predict next word
* Auto-complete sentences
* ChatGPT-like systems

---

### ğŸ”Ÿ Spell Checking & Grammar Correction

* Detect spelling mistakes
* Suggest corrections

---

## 5ï¸âƒ£ Approaches Used in NLP

There are **3 main approaches** used in NLP:

---

### ğŸ”¹ 1. Rule-Based / Heuristic Approach (Old)

* Uses manual rules

**Example**:

* Count positive words vs negative words

**Uses**:

* Regular Expressions
* Dictionaries (WordNet)

âœ… Fast & accurate
âŒ Not scalable

---

### ğŸ”¹ 2. Machine Learning Based Approach

* Data-driven
* Learns patterns automatically
* Text is converted into numbers (vectorization)

**Common algorithms**:

* Naive Bayes
* Logistic Regression
* SVM
* Hidden Markov Models (POS tagging)

âœ… Better than rules
âŒ Needs feature engineering

---

### ğŸ”¹ 3. Deep Learning Based Approach (Modern)

**Big advantages**:

* Understands word order (sequence)
* Automatically learns features

**Popular models**:

* RNN
* LSTM
* GRU
* Transformers

ğŸš€ **Transformers changed NLP completely**.

ğŸ‘‰ All modern models like **BERT, GPT, Gemini** use Transformers.

---

## 6ï¸âƒ£ Challenges in NLP (Very Important)

NLP is hard because **language is complex**.

### âš ï¸ Major Challenges

1ï¸âƒ£ **Ambiguity**

* One sentence â†’ multiple meanings

2ï¸âƒ£ **Context Dependency**

* Same word â†’ different meaning

3ï¸âƒ£ **Slang & Idioms**

* â€œPulling someoneâ€™s legâ€

4ï¸âƒ£ **Sarcasm**

* â€œGreat jobâ€ (could mean bad)

5ï¸âƒ£ **Spelling Mistakes**

* Humans understand, machines struggle

6ï¸âƒ£ **Creativity**

* Poems, metaphors, jokes

7ï¸âƒ£ **Multiple Languages**

* Thousands of languages
* Very little data for many languages

---

## 7ï¸âƒ£ Why NLP is Still Evolving

* Language changes constantly
* New slang appears
* New contexts arise
* New cultures & dialects exist

ğŸ‘‰ We are using **less than 5%** of NLPâ€™s full potential today.

---

## 8ï¸âƒ£ Final Takeaway

* NLP is **powerful but challenging**
* It enables **humanâ€“machine communication**
* Used everywhere in modern technology
* Future growth depends heavily on NLP

---
# ğŸ“˜ NLP Pipeline â€“ Simple English Notes

---

## 1. Why this lecture is important

* NLP is a **difficult topic**
* Before jumping to algorithms, you must understand **how to think**
* This lecture teaches **how to approach any ML / NLP problem**
* In real companies, you donâ€™t just apply models â€” you build **end-to-end systems**

---

## 2. What is an NLP Pipeline?

An **NLP Pipeline** is a **series of steps** followed to build a complete NLP software system.

ğŸ‘‰ You cannot directly apply ML algorithms
ğŸ‘‰ You must go **step by step**

---

## 3. NLP Pipeline â€“ 5 Main Steps

1. **Data Acquisition**
2. **Text Preparation (Preprocessing)**
3. **Feature Engineering**
4. **Modeling + Evaluation**
5. **Deployment**

---

## 4. Step 1: Data Acquisition

### Meaning

Collecting text data for your NLP task.

ğŸ“Œ Without data â†’ **NLP system is impossible**

---

### Data availability scenarios

#### Case 1: Data already available (Internal data)

**Examples:**

* CSV file
* Company database

**What to do:**

* If CSV â†’ directly use it
* If database â†’ talk to data engineering team

---

#### Case 2: Data available externally

**Sources:**

* Public datasets
* Web scraping (using BeautifulSoup)
* APIs (using `requests`)
* PDFs â†’ extract text
* Images â†’ OCR
* Audio â†’ Speech-to-Text

âš ï¸ **Challenges:**

* Noisy data
* Website structure changes
* Unwanted text

---

#### Case 3: No data available

**What to do:**

* Collect data manually (forms, feedback)
* Label data yourself
* Start with rule-based system
* Move to ML later when data increases

---

### Data Augmentation (when data is less)

Create **synthetic data** from existing data.

**Techniques:**

* Synonym replacement
* Word order change
* Back translation
* Adding small noise

**Purpose:**

* Increase dataset size
* Improve model performance

---

## 5. Step 2: Text Preparation (Text Preprocessing)

### Goal

Make text **clean and machine-readable**

---

### Three levels of preprocessing

#### A. Basic Cleaning

* Remove HTML tags
* Handle emojis (Unicode normalization)
* Fix spelling mistakes
* Remove unwanted symbols

---

#### B. Basic Text Preprocessing (Most important)

**Common steps:**

* Tokenization (sentence & word)
* Lowercasing
* Stopword removal
* Punctuation removal
* Digit removal (optional)
* Language detection (optional)

ğŸ“Œ Used in **almost every NLP project**

---

#### C. Advanced Text Preprocessing

Used in complex NLP systems (chatbots, QA systems).

**Techniques:**

* POS Tagging (Part of Speech)
* Parsing (sentence structure)
* Coreference Resolution (he, she, it â†’ who?)

---

## 6. Step 3: Feature Engineering

### Meaning

Convert **text into numbers**.

ğŸ“Œ ML & DL models work **only on numbers**

---

### Simple example

For each review:

* Count positive words
* Count negative words
* Count neutral words

**Text â†’ Numbers â†’ Model**

---

### Common Feature Engineering Techniques

* Bag of Words (BoW)
* TF-IDF
* Word Embeddings
* Deep Learning embeddings

---

### ML vs Deep Learning (Important)

#### Machine Learning

* You manually create features
* Needs domain knowledge
* Results are interpretable

âŒ **Disadvantage:** More effort

---

#### Deep Learning

* Features are learned automatically
* Needs large data
* Works like a black box

âŒ **Disadvantage:** Less interpretability

---

## 7. Step 4: Modeling & Evaluation

### Modeling

Apply algorithms on features.

**Approaches:**

* Rule-based (very less data)
* Machine Learning algorithms
* Deep Learning models
* Cloud APIs (ready-made solutions)

**Choice depends on:**

* Amount of data
* Nature of problem

---

### Evaluation (Very important)

#### Intrinsic Evaluation (Technical)

* Accuracy
* Precision
* Recall
* Confusion Matrix
* Perplexity (for text generation)

---

#### Extrinsic Evaluation (Business)

* User engagement
* Click rate
* Product usage

ğŸ“Œ Good accuracy â‰  good product
ğŸ“Œ Business metrics matter

---

## 8. Step 5: Deployment

Making the model usable by users.

### Deployment stages

#### Deployment

* API / Microservice
* App / Website / Chatbot

#### Monitoring

* Track model performance
* Dashboards
* Detect performance drop

#### Updating

* Retrain with new data
* Replace old models
* Online learning (optional)

---

## 9. Important Points to Remember

* NLP pipeline is **not linear**
* You may go back and forth between steps
* Pipeline differs for ML and Deep Learning
* Real-world projects are **iterative**

---

## 10. Assignment (Given in lecture)

### Problem

Detect **duplicate questions on Quora**.

### You must think about:

* Data source
* Text cleaning steps
* Feature engineering approach
* Algorithm selection
* Evaluation metrics
* Deployment strategy
* Monitoring & updates

ğŸ“Œ No coding required
ğŸ“Œ Focus on **thinking process**

---

## 11. Final Summary

* NLP is not just algorithms
* Pipeline thinking is critical
* **Data â†’ Clean â†’ Features â†’ Model â†’ Deploy**
* This approach helps in **real industry projects**

---
# ğŸ“˜ Text Preprocessing in NLP â€“ Simple English Notes (Part 1)

---

## 1. Introduction: Where Text Preprocessing Fits

In an NLP Pipeline, the main steps are:

* Data Acquisition
* Text Preprocessing âœ… (This video focuses on this)
* Feature Engineering
* Modeling
* Deployment

ğŸ‘‰ Once you collect raw text data, you cannot directly apply ML models.
ğŸ‘‰ You must clean and preprocess the text first.

### Types of Text Preprocessing

* **Basic Text Preprocessing** (covered here)
* **Advanced Text Processing** (POS tagging, parsing, coreference resolution â€“ future videos)

---

## 2. Why Text Preprocessing Is Important

* Raw text is dirty and inconsistent
* Same word may appear in different formats
* Noise increases model complexity

Cleaning text improves:

* Accuracy
* Speed
* Model understanding

âš ï¸ **Important Rule:**
You do NOT apply all steps to every dataset.
You choose steps based on the problem and data type.

---

## 3. Lowercasing

### What is Lowercasing?

Convert all characters in text to lowercase.

### Why?

* â€œBasicâ€ and â€œbasicâ€ should be treated as the same word
* Reduces duplicate vocabulary
* Simplifies model learning

### Example

**Without lowercasing:**

Basic â‰  basic

**With lowercasing:**

basic = basic

### Python Code (Single Text)

```python
text = "This Movie Is AMAZING"
text.lower()
```

### Python Code (Dataset â€“ Pandas)

```python
df['review'] = df['review'].str.lower()
```

ğŸ“Œ This is usually the first step in text preprocessing.

---

## 4. Removing HTML Tags

### Problem

When text is scraped from websites, it often contains HTML tags like:

* `<p>`, `<br>`, `<a>`, `<div>`

These tags:

* Help browsers
* Do NOT help ML models

### Solution: Remove HTML Tags

### Python Code (Using Regex)

```python
import re

def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return re.sub(pattern, '', text)
```

### Example

```python
text = "<p>This movie was <b>great</b></p>"
remove_html_tags(text)
```

âœ… **Output:**

```
This movie was great
```

### Apply to Dataset

```python
df['review'] = df['review'].apply(remove_html_tags)
```

---

## 5. Removing URLs

### Problem

Text from:

* Twitter
* WhatsApp
* Instagram
* Reviews

Often contains URLs like:

* [https://example.com](https://example.com)
* [www.site.com](http://www.site.com)

URLs:

* Add noise
* Do not contribute to sentiment or meaning

### Python Code

```python
def remove_urls(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return re.sub(pattern, '', text)
```

### Example

```python
text = "Check this https://google.com now!"
remove_urls(text)
```

âœ… **Output:**

```
Check this  now!
```

---

## 6. Removing Punctuation

### Why Remove Punctuation?

* Punctuation becomes separate tokens
* Increases vocabulary size unnecessarily
* Confuses ML models

### Example

Hello! â†’ "Hello" + "!"

### Method 1 (Slow â€“ NOT recommended for big data)

```python
import string

def remove_punctuation(text):
    for char in string.punctuation:
        text = text.replace(char, '')
    return text
```

âŒ Very slow for large datasets

### Method 2 (FAST & Recommended)

```python
import string

def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
```

âœ… Much faster
âœ… Best for large datasets

### Apply to Dataset

```python
df['tweet'] = df['tweet'].apply(remove_punctuation)
```

---

## 7. Chat Word Treatment (Short Forms)

### Problem

In chat and social media, people use shortcuts:

| Short Form | Meaning              |
| ---------- | -------------------- |
| GN         | Good Night           |
| IMO        | In My Opinion        |
| IMHO       | In My Honest Opinion |
| BRB        | Be Right Back        |

ML models cannot understand shortcuts.

### Solution

Replace shortcuts using a dictionary

### Example Dictionary

```python
chat_words = {
    "gn": "good night",
    "imo": "in my opinion",
    "imho": "in my honest opinion",
    "brb": "be right back"
}
```

### Python Code

```python
def chat_word_conversion(text):
    words = text.split()
    converted_words = []

    for word in words:
        if word.lower() in chat_words:
            converted_words.append(chat_words[word.lower()])
        else:
            converted_words.append(word)

    return " ".join(converted_words)
```

### Example

```python
text = "IMHO this movie is great GN"
chat_word_conversion(text)
```

âœ… **Output:**

```
in my honest opinion this movie is great good night
```

---

## 8. Spelling Correction

### Problem

Misspellings create multiple versions of the same word

Example:

notebook â‰  noteebok

This:

* Increases vocabulary
* Reduces model accuracy

### Using TextBlob (Simple & Effective)

```python
from textblob import TextBlob

text = "I luvv this moovie"
corrected_text = str(TextBlob(text).correct())
print(corrected_text)
```

âœ… **Output:**

```
I love this movie
```

âš ï¸ **Note:**

* Works well for common English
* For domain-specific data, custom spell checkers may be needed

---

## 9. Removing Stop Words

### What are Stop Words?

Common words that:

* Help sentence formation
* Do NOT add meaning

Examples:

is, am, are, the, a, an, in

### Why Remove?

* Reduce noise
* Improve feature quality

âš ï¸ **Do NOT remove stop words for:**

* POS tagging
* Grammar analysis

### Using NLTK

```python
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
```

### Python Code

```python
def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)
```

### Example

```python
text = "this is a very good movie"
remove_stopwords(text)
```

âœ… **Output:**

```
good movie
```

---

## 10. Handling Emojis

### Problem

ML models cannot understand emojis directly

### Two Options

* Remove emojis âŒ
* Replace emojis with meaning âœ… (Better)

### Option 1: Remove Emojis

```python
import re

def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)
```

### Option 2: Replace Emojis with Meaning (Recommended)

```python
import emoji

def replace_emojis(text):
    return emoji.demojize(text)
```

### Example

```python
text = "I love this movie ğŸ˜ğŸ”¥"
replace_emojis(text)
```

âœ… **Output:**

```
I love this movie :smiling_face_with_heart_eyes: :fire:
```

---

## ğŸ”‘ Key Takeaways

* Text preprocessing is mandatory
* Choose steps based on data
* For large datasets â†’ always prefer efficient methods
* Clean text = Better ML models

  # ğŸ“˜ Text Preprocessing â€“ Part 2

## Tokenization, Stemming & Lemmatization (Simple English Notes)

---

## 1. Tokenization (MOST IMPORTANT STEP)

### What is Tokenization?

Tokenization is the process of breaking text into smaller units called **tokens**.

Tokens can be:

* Words
* Sentences
* Subwords (advanced)

### Example

**Sentence:**

```
I am an Indian
```

**Word Tokens:**

```
["I", "am", "an", "Indian"]
```

**Sentence Tokens:**

```
["I am an Indian.", "I love my country."]
```

---

## 2. Why Tokenization is Important?

* Machine Learning models do not understand raw text
* They work only with numbers

Before converting text into numbers:

* We must split text properly

âŒ Wrong tokenization â†’ wrong features â†’ bad model performance

### Real Example

**Sentence:**

```
I am new in New Delhi
```

If tokenization is wrong:

```
new â‰  New
```

Correct tokenization + lowercasing:

```
["i", "am", "new", "in", "new", "delhi"]
```

âœ… Unique words = 4 (not 5)

ğŸ‘‰ Wrong tokenization confuses the model

---

## 3. Types of Tokenization

### 1ï¸âƒ£ Word Tokenization

* Split sentence into words

### 2ï¸âƒ£ Sentence Tokenization

* Split paragraph into sentences

ğŸ“Œ Choice depends on:

* Your project
* Feature engineering logic

ğŸ“Œ Most NLP tasks use **word tokenization**

---

## 4. Problems in Tokenization (Real-World Challenges)

Tokenization is **NOT** as simple as splitting by space.

### Common Issues:

* **Punctuation** â†’ `Delhi!`
* **Numbers + units** â†’ `5km â†’ 5 + km`
* **Email IDs** â†’ `help@gmail.com`
* **Abbreviations** â†’ `U.S., Ph.D.`
* **Hyphenated words** â†’ `well-known`

ğŸ‘‰ Simple `.split()` fails in these cases

---

## 5. Tokenization Techniques

### ğŸ”¹ Technique 1: Python `split()` (VERY BASIC)

```python
text = "I am going to Delhi!"
tokens = text.split()
print(tokens)
```

âŒ Output:

```
['I', 'am', 'going', 'to', 'Delhi!']
```

âŒ Problem:

```
Delhi! â‰  Delhi
```

ğŸ“Œ Use only for very clean text

---

### ğŸ”¹ Technique 2: Regular Expressions (Better but Complex)

```python
import re

text = "I am going to Delhi!"
tokens = re.findall(r'\w+', text)
print(tokens)
```

âœ… Output:

```
['I', 'am', 'going', 'to', 'Delhi']
```

âš ï¸ Still has problems with:

* Emails
* Units (5km)
* Abbreviations

---

### ğŸ”¹ Technique 3: NLTK Tokenizer (GOOD)

#### Word Tokenization

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

text = "I am going to Delhi!"
tokens = word_tokenize(text)
print(tokens)
```

âœ… Output:

```
['I', 'am', 'going', 'to', 'Delhi', '!']
```

#### Sentence Tokenization

```python
from nltk.tokenize import sent_tokenize

text = "I love India. Delhi is my city!"
sentences = sent_tokenize(text)
print(sentences)
```

âœ… Output:

```
['I love India.', 'Delhi is my city!']
```

âš ï¸ NLTK is good, but not perfect

---

### ğŸ”¹ Technique 4: spaCy Tokenizer (BEST â­)

ğŸ“Œ Industry-recommended tokenizer

Handles:

* Emails
* Numbers + units
* Abbreviations
* Punctuation
* Complex grammar

#### Installation

```bash
pip install spacy
python -m spacy download en_core_web_sm
```

#### Word Tokenization using spaCy

```python
import spacy

nlp = spacy.load("en_core_web_sm")

text = "Email us at help@gmail.com or walk 5km today!"
doc = nlp(text)

tokens = [token.text for token in doc]
print(tokens)
```

âœ… Output:

```
['Email', 'us', 'at', 'help@gmail.com', 'or', 'walk', '5', 'km', 'today', '!']
```

ğŸ“Œ spaCy is the best choice for real projects

---

## 6. Key Takeaway on Tokenization

* Tokenization is NOT trivial
* Wrong tokens â†’ wrong features
* No tokenizer is 100% perfect

ğŸ“Œ Use:

* **spaCy** â†’ complex / real-world text
* **NLTK** â†’ simple tasks

---

## ğŸŒ± Stemming

## 7. What is Stemming?

Stemming reduces words to their root form by removing prefixes/suffixes.

âš ï¸ The root word may NOT be a real English word.

### Example

| Word    | Stem  |
| ------- | ----- |
| working | work  |
| worked  | work  |
| studies | studi |
| movies  | movi  |

---

## 8. Why Use Stemming?

* Reduces vocabulary size
* Groups similar words
* Useful in Information Retrieval (Search Engines)

---

## 9. Stemming using NLTK (Porter Stemmer)

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

text = "working worked works"
tokens = word_tokenize(text)

stemmed_words = [stemmer.stem(word) for word in tokens]
print(stemmed_words)
```

âœ… Output:

```
['work', 'work', 'work']
```

âš ï¸ Problem:

```
studies â†’ studi
```

---

## ğŸƒ Lemmatization

## 10. What is Lemmatization?

Lemmatization converts words to their dictionary (meaningful) base form.

âœ… Output is always a real English word

---

## 11. Difference: Stemming vs Lemmatization

| Feature  | Stemming       | Lemmatization    |
| -------- | -------------- | ---------------- |
| Speed    | Fast           | Slow             |
| Output   | Not real words | Real words       |
| Logic    | Rule-based     | Dictionary-based |
| Accuracy | Lower          | Higher           |

---

## 12. Lemmatization using NLTK (WordNet)

```python
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

text = "movies are running better"
tokens = word_tokenize(text)

lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]
print(lemmatized_words)
```

âœ… Output:

```
['movie', 'are', 'running', 'better']
```

ğŸ“Œ Better results when POS tag is provided

---

## 13. When to Use What?

### Use **Stemming** when:

* Speed matters
* Output not shown to user
* Search engines, IR systems

### Use **Lemmatization** when:

* Output shown to user
* Grammar matters
* Chatbots, summaries, NLP apps

---

## 14. Final Summary

âœ… Tokenization is the foundation of NLP

âœ… spaCy gives the best tokenization

âœ… Stemming is fast but rough

âœ… Lemmatization is slow but accurate

âœ… Choose preprocessing steps based on use-case

---

## ğŸ“ Assignment (From Video)

Create your **own dataset** (do NOT use ready datasets):

**Movie Dataset Columns:**

* Movie Name
* Description (text)
* Genre (label)

Apply the following steps:

* Lowercasing
* Remove HTML
* Remove URLs
* Remove punctuation
* Stopwords removal
* Tokenization
* Stemming / Lemmatization

ğŸ“Œ Learning matters more than using ready datasets

# ğŸ“˜ Text Representation / Feature Extraction from Text

*(Simple English + Deep Explanation Notes â€“ Part 1)*

---

## 1ï¸âƒ£ Where This Topic Fits in NLP Pipeline

Recall the NLP Pipeline:

1. Data Collection
2. Text Preprocessing
3. **Text Representation / Feature Extraction âœ… (THIS VIDEO)**
4. Modeling
5. Evaluation
6. Deployment

ğŸ‘‰ After preprocessing, text must be converted into numbers.
ğŸ‘‰ Machine Learning algorithms do **not** understand text; they understand **numbers only**.

---

## 2ï¸âƒ£ What is Feature Extraction from Text?

### Simple Definition

Feature Extraction from Text means:

> Converting raw text into numerical features so that machine learning algorithms can process it.

It is also called:

* Text Representation
* Text Vectorization

### Why â€œVectorizationâ€?

Because text is finally converted into **vectors (arrays of numbers)**.

### Example Problem â€“ Sentiment Analysis

Text:

> "This movie is very good"

Output:

* Positive / Negative

âš ï¸ ML models do **not** understand English words.
They understand **math and numbers**.

So the flow is:

> Text â†’ Numbers â†’ ML Model â†’ Output

---

## 3ï¸âƒ£ Why Do We Need Feature Extraction?

### Famous ML Rule

> **Garbage In â†’ Garbage Out**

* Bad features â†’ Bad output
* Good features â†’ Good output

Even:

> A simple algorithm + good features
> can outperform
> a complex algorithm + bad features

### Text Example (Sentiment Analysis)

Possible numerical features:

* Number of positive words
* Number of negative words
* Length of sentence
* Average word length

ğŸ‘‰ These act as **numeric signals** for ML models.

---

## 4ï¸âƒ£ Why is Text â†’ Numbers Difficult?

### Compare with Other Data Types

#### ğŸ–¼ï¸ Image Data

* Image = Pixels
* Pixels = Numbers
  âœ” Easy to convert

#### ğŸ”Š Audio Data

* Audio = Waveform
* Waveform = Amplitude values
  âœ” Easy to convert

#### ğŸ“ Text Data

Example:

> "Hello, how are you?"

Problems:

* âŒ No direct numeric form
* âŒ Meaning is abstract
* âŒ Order, context, emotion matter

ğŸ‘‰ Thatâ€™s why **text representation is hard**.

---

## 5ï¸âƒ£ Core Idea Behind Text Representation (VERY IMPORTANT)

While converting text to numbers,
**semantic meaning must be preserved**.

If numbers do not capture meaning:
â†’ Model performance will be poor.

Success depends on:

* How well meaning is encoded
* Similar texts getting similar vectors
* Different texts getting distant vectors

---

## 6ï¸âƒ£ Popular Text Representation Techniques

### Techniques Overview

* One Hot Encoding
* Bag of Words (BoW)
* N-grams
* TF-IDF
* Custom Hand-crafted Features
* Word Embeddings (Word2Vec, GloVe) *(Next video)*
* Deep Learning embeddings *(Later)*

ğŸ‘‰ This video covers:

* One Hot Encoding
* Bag of Words

---

## 7ï¸âƒ£ Important Terminology (Very Important)

### Corpus

All text data combined together.

### Vocabulary

Set of **unique words** in the corpus.

### Document

One individual text (review, sentence, email).

### Word / Token

Single unit inside a document.

### Example (IMDB Dataset)

* 50,000 reviews â†’ 50,000 documents
* Each review = one document
* All words together = corpus
* Unique words = vocabulary

---

## 8ï¸âƒ£ One Hot Encoding (First Technique)

### Idea

Each word is represented as a **binary vector**.

### Example Dataset

| Document | Sentence             |
| -------- | -------------------- |
| D1       | people watch campus  |
| D2       | watch campus campus  |
| D3       | people write comment |
| D4       | campus write comment |

### Step 1: Build Vocabulary

Vocabulary = [people, watch, campus, write, comment]

Size = 5

### Step 2: One Hot Representation

| Word    | Vector      |
| ------- | ----------- |
| people  | [1,0,0,0,0] |
| watch   | [0,1,0,0,0] |
| campus  | [0,0,1,0,0] |
| write   | [0,0,0,1,0] |
| comment | [0,0,0,0,1] |

### Document Representation

A document is represented by combining vectors of its words.

---

## 9ï¸âƒ£ Problems with One Hot Encoding âŒ

### âŒ 1. Sparse Vectors

* Vocabulary size = 50,000
* Vector length = 50,000
* Only one `1`, rest are `0`

ğŸ‘‰ High memory and computation cost

### âŒ 2. Not Fixed Input Size

* Different sentences â†’ different shapes
* ML models need **fixed-size input**

### âŒ 3. Out-of-Vocabulary (OOV)

* New word at prediction time â†’ model fails

### âŒ 4. No Semantic Meaning

Example words:

* run, walk, bottle

But:

* run & walk are similar
* bottle is different

One-hot treats all equally âŒ

### Final Verdict

* âŒ Not used in real applications
* âœ” Used only to understand basics

---

## 1ï¸âƒ£0ï¸âƒ£ Bag of Words (BoW) â€“ MOST IMPORTANT

### Core Idea

Instead of asking:

> Is the word present?

Ask:

> How many times does the word appear?

### Vocabulary (Same as Before)

[people, watch, campus, write, comment]

### BoW Representation

| Document | people | watch | campus | write | comment |
| -------- | ------ | ----- | ------ | ----- | ------- |
| D1       | 1      | 1     | 1      | 0     | 0       |
| D2       | 0      | 1     | 2      | 0     | 0       |
| D3       | 1      | 0     | 0      | 1     | 1       |
| D4       | 0      | 0     | 1      | 1     | 1       |

âœ” Fixed size vectors
âœ” Works for new sentences
âœ” Widely used in text classification

---

## 1ï¸âƒ£1ï¸âƒ£ Intuition Behind Bag of Words

Documents with:

* Similar words
* Similar frequencies

â†’ have similar meaning

### Vector Space View

* Each document = a point in high-dimensional space
* Similarity measured using:

  * Cosine similarity
  * Distance metrics

---

## 1ï¸âƒ£2ï¸âƒ£ Bag of Words Implementation (sklearn)

```python
from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "people watch campus",
    "watch campus campus",
    "people write comment",
    "campus write comment"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

print(vectorizer.vocabulary_)
print(X.toarray())
```

### Output

**Vocabulary:**

```
{'people':0, 'watch':1, 'campus':2, 'write':3, 'comment':4}
```

**BoW Matrix:**

```
[[1 1 1 0 0]
 [0 1 2 0 0]
 [1 0 0 1 1]
 [0 0 1 1 1]]
```

---

## 1ï¸âƒ£3ï¸âƒ£ Important Parameters of CountVectorizer

* `lowercase=True`

  * Converts all text to lowercase

* `stop_words='english'`

  * Removes common words like: is, the, and, of

* `binary=True`

  * Uses only presence (1) or absence (0)
  * Useful in sentiment analysis

* `max_features`

  * Keeps only top-N frequent words

  ```python
  CountVectorizer(max_features=1000)
  ```

---

## 1ï¸âƒ£4ï¸âƒ£ Advantages of Bag of Words âœ…

* âœ” Simple and intuitive
* âœ” Fixed-size input
* âœ” Handles unseen sentences
* âœ” Works well for text classification
* âœ” Better than One Hot Encoding

---

## 1ï¸âƒ£5ï¸âƒ£ Disadvantages of Bag of Words âŒ

### âŒ 1. Sparse Vectors

Large vocabulary â†’ many zeros

### âŒ 2. Order Ignored

"movie is good"
"good is movie"

Same vector âŒ

### âŒ 3. Context Ignored

"This is good movie"
"This is not good movie"

Almost same vector âŒ

### âŒ 4. Weak Semantic Understanding

Meaning is only partially captured

---

## 1ï¸âƒ£6ï¸âƒ£ Why We Need Next Technique (N-grams)

Problems with BoW:

* Word order ignored
* Context loss
* Negation not handled properly

ğŸ‘‰ Next technique: **N-grams**

---

## âœ… Summary of Part-1

| Technique        | Used in Practice |
| ---------------- | ---------------- |
| One Hot Encoding | âŒ No             |
| Bag of Words     | âœ… Yes            |
| N-grams          | âœ… Yes            |
| TF-IDF           | âœ… Yes            |
| Word Embeddings  | âœ… Yes            |

---

ğŸ“Œ **End of Part-1: Text Representation Basics**

# ğŸ“˜ Text Representation â€“ N-grams, TF-IDF & Custom Features

(Simple English + Conceptual Notes â€“ Part 2)

---

## 1ï¸âƒ£ What are N-grams?

### Definition

N-grams are continuous sequences of **N words** taken from a sentence.

* **Unigram (1-gram)** â†’ single word
* **Bigram (2-gram)** â†’ two continuous words
* **Trigram (3-gram)** â†’ three continuous words

This technique is also called **Bag of N-grams**.

### Why N-grams are Needed?

Earlier techniques (Bag of Words / Unigram) had problems:

* âŒ Ignore word order
* âŒ Miss important phrases like:

  * "not good"
  * "very bad"
  * "no improvement"

ğŸ‘‰ Word order matters in language.
ğŸ‘‰ N-grams solve this problem **partially**.

---

## 2ï¸âƒ£ How N-grams Work (Intuition)

### Sentence

```
people watch campus
```

### Unigrams

```
people | watch | campus
```

### Bigrams

```
people watch | watch campus
```

### Trigrams

```
people watch campus
```

âš ï¸ **Important Rule:**

* Words must be **continuous** (no skipping words).

---

## 3ï¸âƒ£ Example Dataset

| Document | Sentence             |
| -------- | -------------------- |
| D1       | people watch campus  |
| D2       | watch campus campus  |
| D3       | people write comment |
| D4       | campus write comment |

### Bigram Vocabulary (Bag of Bigrams)

* people watch
* watch campus
* campus campus
* people write
* write comment
* campus write

ğŸ‘‰ Vocabulary size increases compared to unigrams.

---

## 4ï¸âƒ£ Vector Representation with Bigrams

Each document is represented using **frequency of bigrams**.

### Example for D1: "people watch campus"

* people watch â†’ 1
* watch campus â†’ 1
* others â†’ 0

âœ” Fixed-size vector
âœ” Captures local word order

---

## 5ï¸âƒ£ N-grams using Scikit-Learn

### Important Parameter: `ngram_range`

```python
from sklearn.feature_extraction.text import CountVectorizer

# Unigram (Bag of Words)
CountVectorizer(ngram_range=(1,1))

# Bigram
CountVectorizer(ngram_range=(2,2))

# Unigram + Bigram
CountVectorizer(ngram_range=(1,2))

# Trigram
CountVectorizer(ngram_range=(3,3))
```

### Example Code (Bigram)

```python
documents = [
    "people watch campus",
    "watch campus campus",
    "people write comment",
    "campus write comment"
]

vectorizer = CountVectorizer(ngram_range=(2,2))
X = vectorizer.fit_transform(documents)

print(vectorizer.vocabulary_)
print(X.toarray())
```

---

## 6ï¸âƒ£ Why N-grams are Better than Bag of Words?

### Key Example

**Sentence 1:**

```
This is a very good movie
```

**Sentence 2:**

```
This is not a good movie
```

### Problem with Unigrams / BoW

Both sentences contain:

```
this, is, good, movie
```

ğŸ‘‰ Vectors look very similar
ğŸ‘‰ Meanings are opposite

### Using Bigrams

* Sentence 1 bigram â†’ **very good**
* Sentence 2 bigram â†’ **not good**

âœ” Meaning difference is captured
âœ” Vectors move farther apart in vector space

---

## 7ï¸âƒ£ Benefits of N-grams âœ…

* Captures word order (local context)
* Handles negation ("not good")
* Better semantic representation than unigrams
* Easy to understand and implement
* Very useful in:

  * Sentiment Analysis
  * Text Classification

---

## 8ï¸âƒ£ Disadvantages of N-grams âŒ

### 1. Vocabulary Explosion

* Unigrams â†’ small vocabulary
* Bigrams â†’ larger
* Trigrams â†’ very large

ğŸ‘‰ High memory and computation cost

### 2. Sparse Vectors

* More features â†’ more zeros
* Hard to handle large datasets

### 3. Out-of-Vocabulary (OOV) Problem

* New phrase at prediction time â†’ ignored

### 4. No Deep Semantic Understanding

Example:

```
beautiful â‰  gorgeous
```

N-grams treat them as different words.

ğŸ‘‰ This is solved using **Word Embeddings**.

---

## 9ï¸âƒ£ Summary Till Now

| Technique    | Order     | Context    | Used in Practice |
| ------------ | --------- | ---------- | ---------------- |
| One Hot      | âŒ         | âŒ          | âŒ                |
| Bag of Words | âŒ         | âŒ          | âœ…                |
| N-grams      | âœ… (local) | âš ï¸ partial | âœ…                |

---

## ğŸ”Ÿ TF-IDF (Why We Need It)

### Problem with BoW & N-grams

* All words treated as equally important

Example words:

```
people, watch, campus
```

But in reality:

* Some words are important
* Some words are common and useless

ğŸ‘‰ **TF-IDF fixes this problem**.

---

## 1ï¸âƒ£1ï¸âƒ£ What is TF-IDF?

**TF-IDF = Term Frequency Ã— Inverse Document Frequency**

It gives higher weight to:

* Words frequent in a document
* Words rare across the corpus

---

## 1ï¸âƒ£2ï¸âƒ£ Term Frequency (TF)

### Formula

```
TF(word) = (Number of times word appears in document)
           -------------------------------------------
           (Total words in document)
```

ğŸ‘‰ Measures importance **inside a document**

---

## 1ï¸âƒ£3ï¸âƒ£ Inverse Document Frequency (IDF)

### Formula

```
IDF(word) = log(
    Total number of documents
    -------------------------
    Number of documents containing the word
)
```

ğŸ‘‰ Measures **rarity across corpus**

### Intuition

* Word in every document â†’ IDF â‰ˆ 0
* Word in few documents â†’ IDF is high

---

## 1ï¸âƒ£4ï¸âƒ£ Why Log is Used in IDF?

* Without log â†’ rare words get very large values
* TF becomes meaningless

Log function:

* Smooths extreme values
* Balances TF and IDF

---

## 1ï¸âƒ£5ï¸âƒ£ TF-IDF Final Formula

```
TF-IDF(word) = TF(word) Ã— IDF(word)
```

ğŸ‘‰ Final value shows:

* Importance in the document
* Importance in the corpus

---

## 1ï¸âƒ£6ï¸âƒ£ TF-IDF using Scikit-Learn

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

print(vectorizer.vocabulary_)
print(X.toarray())
```

### Note on sklearn IDF

Scikit-learn uses **smoothed IDF**:

```
idf = log((N + 1) / (df + 1)) + 1
```

* Prevents zero values
* This is only an implementation detail

---

## 1ï¸âƒ£7ï¸âƒ£ Where TF-IDF is Used?

* Search engines (Google)
* Information Retrieval
* Text Classification
* Document Similarity

---

## 1ï¸âƒ£8ï¸âƒ£ Custom Features (Hand-crafted Features)

Sometimes built-in techniques are not enough.

ğŸ‘‰ We create **custom features** using domain knowledge.

### Examples (Sentiment Analysis)

* Number of positive words
* Number of negative words
* Ratio of positive / negative words
* Total word count
* Average word length
* Character count

### Why Custom Features Matter?

* You understand the problem better than the algorithm
* Domain knowledge improves accuracy

---

## 1ï¸âƒ£9ï¸âƒ£ Hybrid Features (Very Important)

In real-world ML projects, we use:

```
BoW / N-grams / TF-IDF
+
Custom Features
```

ğŸ‘‰ This is called **Hybrid Features**
ğŸ‘‰ Used in **99% of real ML projects**

---

## 2ï¸âƒ£0ï¸âƒ£ Final Big Picture

| Technique       | Solves What                   |
| --------------- | ----------------------------- |
| BoW             | Fixed size, frequency         |
| N-grams         | Word order, phrases           |
| TF-IDF          | Word importance               |
| Custom Features | Domain intelligence           |
| Word Embeddings | Semantic meaning (NEXT VIDEO) |

---

ğŸ“Œ **Next Topic:** Word Embeddings (Word2Vec, GloVe, FastText)

# ğŸ“˜ Word Embeddings & Word2Vec (Simple English Notes)

## 1ï¸âƒ£ Where this Topic Fits in NLP

In the NLP pipeline:

1. Data Collection
2. Text Preprocessing
3. **Text Representation / Feature Extraction âœ…**
4. Modeling
5. Deployment

ğŸ‘‰ **Todayâ€™s topic:** Advanced Text Representation â€“ **Word Embeddings & Word2Vec**

---

## 2ï¸âƒ£ What is Word Embedding?

### Simple Definition

Word Embedding is a technique where:

* Each word is converted into a **vector (numbers)**
* **Similar words get similar vectors**

### Example

```
king  â†’ [0.21, -0.33, 0.58, ...]
queen â†’ [0.20, -0.31, 0.60, ...]
```

ğŸ‘‰ These vectors capture **meaning**, not just frequency.

---

## 3ï¸âƒ£ Why Word Embeddings are Needed?

### Old Techniques

* Bag of Words (BoW)
* TF-IDF

### Problems with Old Techniques

âŒ No semantic meaning
âŒ Very high dimensions
âŒ Sparse vectors (many zeros)

### How Word Embeddings Help

âœ… Capture semantic meaning
âœ… Low-dimensional vectors
âœ… Dense vectors (fewer zeros)
âœ… Faster computation

---

## 4ï¸âƒ£ Types of Word Embedding Techniques

### ğŸ”¹ 1. Frequency-based Embeddings

* Bag of Words
* TF-IDF
* GloVe

ğŸ‘‰ Based on **word frequency**

### ğŸ”¹ 2. Prediction-based Embeddings

* **Word2Vec âœ…**
* FastText

ğŸ‘‰ Based on **predicting words using neural networks**

---

## 5ï¸âƒ£ What is Word2Vec?

### Definition

Word2Vec is a **prediction-based word embedding technique** that:

* Converts words into vectors
* Uses neural networks
* Learns meaning from context

ğŸ“Œ Developed by **Google (2013)**

---

## 6ï¸âƒ£ Why Word2Vec is Better than BoW / TF-IDF?

### âœ… Major Advantages

#### 1ï¸âƒ£ Semantic Meaning

* "happy" â‰ˆ "joy"
* BoW / TF-IDF cannot capture this

#### 2ï¸âƒ£ Low Dimensional

* Typically **100â€“300 dimensions**
* Not thousands like BoW

#### 3ï¸âƒ£ Dense Vectors

* Very few zero values
* Less overfitting

ğŸ‘‰ **Interview Answer Ready âœ”ï¸**

---

## 7ï¸âƒ£ Vector Properties of Word2Vec

* ğŸ”¹ Similar words â†’ close vectors
* ğŸ”¹ Different words â†’ far vectors

### You Can Do:

* Similarity calculation
* Vector arithmetic

### Example

```
king - man + woman â‰ˆ queen
```

---

## 8ï¸âƒ£ Using Pre-trained Word2Vec (Google News)

### Pre-trained Model Details

* Trained on **Google News**
* ~3 million words
* 300-dimensional vectors
* File size ~1.5GB

### ğŸ”§ Install Required Libraries

```bash
pip install gensim nltk
```

### ğŸ“Œ Load Pre-trained Word2Vec Model

```python
import gensim
from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format(
    "GoogleNews-vectors-negative300.bin",
    binary=True
)
```

### ğŸ”¹ Get Vector of a Word

```python
vector = model["man"]
print(vector)
print(vector.shape)
```

**Output:**

```
(300,)
```

### ğŸ”¹ Most Similar Words

```python
model.most_similar("cricket")
```

### ğŸ”¹ Similarity Between Two Words

```python
model.similarity("man", "woman")
```

### ğŸ”¹ Odd One Out

```python
model.doesnt_match(["php", "java", "python", "banana"])
```

### ğŸ”¹ Vector Arithmetic

```python
model.most_similar(positive=["woman", "king"], negative=["man"])
```

---

## 9ï¸âƒ£ Important Intuition Behind Word2Vec

### ğŸ”‘ Core Idea

> Words used in **similar contexts** have **similar meanings**

### Example

* football player
* hockey player

ğŸ‘‰ Word2Vec learns:

```
football â‰ˆ hockey
```

---

## ğŸ”Ÿ Two Architectures of Word2Vec

### ğŸ”¹ 1. CBOW (Continuous Bag of Words)

* Predicts **target word** from context
* Faster
* Good for small datasets

**Context â†’ Target**

### ğŸ”¹ 2. Skip-Gram

* Predicts **context words** from target
* Slower
* Better for large datasets

**Target â†’ Context**

---

## 1ï¸âƒ£1ï¸âƒ£ Training Data Creation (Sliding Window)

### Sentence

```
watch campus for data science
```

### Window Size = 3

#### CBOW Training Samples

| Input (Context) | Output (Target) |
| --------------- | --------------- |
| watch, for      | campus          |
| campus, data    | for             |
| for, science    | data            |

---

## 1ï¸âƒ£2ï¸âƒ£ Neural Network Structure (Conceptual)

* **Input:** One-hot vectors
* **Hidden Layer:** Embedding layer (size = embedding dimension)
* **Output:** Softmax over vocabulary
* **Loss:** Cross-entropy
* **Training:** Backpropagation

ğŸ‘‰ **Embeddings = weights of hidden layer**

---

## 1ï¸âƒ£3ï¸âƒ£ Training Word2Vec on Your Own Data (Gensim)

### ğŸ“‚ Dataset Example

* Game of Thrones books (text files)

### ğŸ”¹ Preprocessing

```python
from gensim.utils import simple_preprocess
from nltk.tokenize import sent_tokenize
import os

story = []

for file in os.listdir("data"):
    with open(f"data/{file}", "r", encoding="utf-8") as f:
        text = f.read()
        sentences = sent_tokenize(text)
        for sent in sentences:
            story.append(simple_preprocess(sent))
```

### ğŸ”¹ Train Word2Vec Model

```python
from gensim.models import Word2Vec

model = Word2Vec(
    sentences=story,
    vector_size=100,
    window=10,
    min_count=2,
    workers=4,
    epochs=10
)
```

### ğŸ”¹ Similar Words (Custom Model)

```python
model.wv.most_similar("daenerys")
```

### ğŸ”¹ Word Vector

```python
model.wv["jon"]
```

### ğŸ”¹ Similarity Check

```python
model.wv.similarity("arya", "sansa")
```

---

## 1ï¸âƒ£4ï¸âƒ£ Visualizing Word Embeddings (PCA)

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

words = list(model.wv.index_to_key)[:100]
vectors = [model.wv[word] for word in words]

pca = PCA(n_components=2)
reduced = pca.fit_transform(vectors)

plt.scatter(reduced[:,0], reduced[:,1])

for i, word in enumerate(words):
    plt.annotate(word, (reduced[i,0], reduced[i,1]))

plt.show()
```

---

## 1ï¸âƒ£5ï¸âƒ£ How to Improve Word2Vec Quality?

âœ” Increase training data
âœ” Increase embedding dimensions
âœ” Increase window size
âœ” Use Skip-Gram for large datasets
âœ” Proper preprocessing

---

## 1ï¸âƒ£6ï¸âƒ£ Interview Quick Answers

**Q: Why Word2Vec over TF-IDF?**
â¡ Captures semantic meaning, dense vectors, low dimension

**Q: CBOW vs Skip-Gram?**
â¡ CBOW â†’ small data
â¡ Skip-Gram â†’ large data

**Q: What does Word2Vec actually learn?**
â¡ Context-based word relationships

---

## 1ï¸âƒ£7ï¸âƒ£ Assignment (Important)

ğŸ‘‰ Choose any text dataset:

* Movie scripts
* News articles
* Friends / GOT scripts

### Tasks

âœ” Train Word2Vec
âœ” Check similarity
âœ” Try vector arithmetic
âœ” Visualize embeddings

---

## âœ… Final Summary

* Word2Vec is a **deep learningâ€“based word embedding**
* Converts **words â†’ vectors**
* Learns **meaning from context**
* Two models: **CBOW & Skip-Gram**
* **Gensim** makes implementation easy
* Very important for **NLP, ML & Interviews**







