# üìò Introduction to Natural Language Processing (NLP)

## Simple English Notes

---

## 1Ô∏è‚É£ What is NLP?

**Natural Language Processing (NLP)** is a field of study that helps computers **understand, process, and generate human language**.

It is a combination of:

* **Linguistics** ‚Üí understanding language rules
* **Computer Science** ‚Üí programming & algorithms
* **Artificial Intelligence (AI)** ‚Üí making machines intelligent

### üîë Main Goal of NLP

To make machines:

* Understand what humans say or write
* Respond back in **natural human language**

NLP is not only about understanding text, but also about **speaking, replying, and interacting naturally**.

---

## 2Ô∏è‚É£ Why is NLP Important? (Need of NLP)

Humans became powerful mainly because of:

* Communication
* Language

We shared ideas, taught future generations, and improved over time.

### üöÄ Next Big Revolution

The next big step in human progress is:

> **Talking to machines the same way we talk to humans**

### Evolution of Interaction

* **Earlier**:

  * Machines ‚Üí buttons, switches
  * Computers ‚Üí programming languages
  * Smartphones ‚Üí touch

* **Now**:

  * Voice
  * Text
  * Conversation

### Examples

* Google Assistant
* Siri
* Alexa
* Chatbots

üëâ NLP makes machines **friendly, usable, and intelligent** for everyone.

---

## 3Ô∏è‚É£ Real-World Applications of NLP

### üü¢ Everyday Applications

* Chatbots (customer support)
* Voice assistants
* Google Search smart answers
* Auto-reply emails
* Spam email detection

### üü¢ Business & Industry

* Targeted advertisements
* Product review analysis
* Customer feedback analysis

### üü¢ Social Media

* Hate-speech detection
* Adult content filtering
* Sentiment analysis on tweets
* Trend & opinion mining

### üü¢ Search Engines

* Direct answers (no need to open websites)
* Knowledge graphs
* Smart suggestions

---

## 4Ô∏è‚É£ Common NLP Tasks (Very Important)

If you want to become an **NLP Engineer**, these tasks are core skills:

### 1Ô∏è‚É£ Text / Document Classification

Classifying text into categories:

* Sports
* Politics
* Technology
* Spam / Not spam

---

### 2Ô∏è‚É£ Sentiment Analysis

Finding emotions from text:

* Positive
* Negative
* Neutral

Used in:

* Product reviews
* Movie reviews
* Social media

---

### 3Ô∏è‚É£ Information Extraction

Extracting useful information:

* Names
* Locations
* Dates
* Product names

---

### 4Ô∏è‚É£ Parts of Speech (POS) Tagging

Assigning labels to words:

* Noun
* Verb
* Adjective

Helps machines understand **sentence structure**.

---

### 5Ô∏è‚É£ Language Detection & Translation

* Detect language automatically
* Translate text (Google Translate)

---

### 6Ô∏è‚É£ Speech to Text & Text to Speech

* Voice ‚Üí Text
* Text ‚Üí Voice

Used in:

* Voice assistants
* Accessibility tools

---

### 7Ô∏è‚É£ Text Summarization

* Convert long text into short summary
* Example: News apps like **Inshorts**

---

### 8Ô∏è‚É£ Topic Modeling

Finding main topics in large text data

**Example**:

* Cricket article ‚Üí IPL, players, matches

---

### 9Ô∏è‚É£ Text Generation

* Predict next word
* Auto-complete sentences
* ChatGPT-like systems

---

### üîü Spell Checking & Grammar Correction

* Detect spelling mistakes
* Suggest corrections

---

## 5Ô∏è‚É£ Approaches Used in NLP

There are **3 main approaches** used in NLP:

---

### üîπ 1. Rule-Based / Heuristic Approach (Old)

* Uses manual rules

**Example**:

* Count positive words vs negative words

**Uses**:

* Regular Expressions
* Dictionaries (WordNet)

‚úÖ Fast & accurate
‚ùå Not scalable

---

### üîπ 2. Machine Learning Based Approach

* Data-driven
* Learns patterns automatically
* Text is converted into numbers (vectorization)

**Common algorithms**:

* Naive Bayes
* Logistic Regression
* SVM
* Hidden Markov Models (POS tagging)

‚úÖ Better than rules
‚ùå Needs feature engineering

---

### üîπ 3. Deep Learning Based Approach (Modern)

**Big advantages**:

* Understands word order (sequence)
* Automatically learns features

**Popular models**:

* RNN
* LSTM
* GRU
* Transformers

üöÄ **Transformers changed NLP completely**.

üëâ All modern models like **BERT, GPT, Gemini** use Transformers.

---

## 6Ô∏è‚É£ Challenges in NLP (Very Important)

NLP is hard because **language is complex**.

### ‚ö†Ô∏è Major Challenges

1Ô∏è‚É£ **Ambiguity**

* One sentence ‚Üí multiple meanings

2Ô∏è‚É£ **Context Dependency**

* Same word ‚Üí different meaning

3Ô∏è‚É£ **Slang & Idioms**

* ‚ÄúPulling someone‚Äôs leg‚Äù

4Ô∏è‚É£ **Sarcasm**

* ‚ÄúGreat job‚Äù (could mean bad)

5Ô∏è‚É£ **Spelling Mistakes**

* Humans understand, machines struggle

6Ô∏è‚É£ **Creativity**

* Poems, metaphors, jokes

7Ô∏è‚É£ **Multiple Languages**

* Thousands of languages
* Very little data for many languages

---

## 7Ô∏è‚É£ Why NLP is Still Evolving

* Language changes constantly
* New slang appears
* New contexts arise
* New cultures & dialects exist

üëâ We are using **less than 5%** of NLP‚Äôs full potential today.

---

## 8Ô∏è‚É£ Final Takeaway

* NLP is **powerful but challenging**
* It enables **human‚Äìmachine communication**
* Used everywhere in modern technology
* Future growth depends heavily on NLP

---
# üìò NLP Pipeline ‚Äì Simple English Notes

---

## 1. Why this lecture is important

* NLP is a **difficult topic**
* Before jumping to algorithms, you must understand **how to think**
* This lecture teaches **how to approach any ML / NLP problem**
* In real companies, you don‚Äôt just apply models ‚Äî you build **end-to-end systems**

---

## 2. What is an NLP Pipeline?

An **NLP Pipeline** is a **series of steps** followed to build a complete NLP software system.

üëâ You cannot directly apply ML algorithms
üëâ You must go **step by step**

---

## 3. NLP Pipeline ‚Äì 5 Main Steps

1. **Data Acquisition**
2. **Text Preparation (Preprocessing)**
3. **Feature Engineering**
4. **Modeling + Evaluation**
5. **Deployment**

---

## 4. Step 1: Data Acquisition

### Meaning

Collecting text data for your NLP task.

üìå Without data ‚Üí **NLP system is impossible**

---

### Data availability scenarios

#### Case 1: Data already available (Internal data)

**Examples:**

* CSV file
* Company database

**What to do:**

* If CSV ‚Üí directly use it
* If database ‚Üí talk to data engineering team

---

#### Case 2: Data available externally

**Sources:**

* Public datasets
* Web scraping (using BeautifulSoup)
* APIs (using `requests`)
* PDFs ‚Üí extract text
* Images ‚Üí OCR
* Audio ‚Üí Speech-to-Text

‚ö†Ô∏è **Challenges:**

* Noisy data
* Website structure changes
* Unwanted text

---

#### Case 3: No data available

**What to do:**

* Collect data manually (forms, feedback)
* Label data yourself
* Start with rule-based system
* Move to ML later when data increases

---

### Data Augmentation (when data is less)

Create **synthetic data** from existing data.

**Techniques:**

* Synonym replacement
* Word order change
* Back translation
* Adding small noise

**Purpose:**

* Increase dataset size
* Improve model performance

---

## 5. Step 2: Text Preparation (Text Preprocessing)

### Goal

Make text **clean and machine-readable**

---

### Three levels of preprocessing

#### A. Basic Cleaning

* Remove HTML tags
* Handle emojis (Unicode normalization)
* Fix spelling mistakes
* Remove unwanted symbols

---

#### B. Basic Text Preprocessing (Most important)

**Common steps:**

* Tokenization (sentence & word)
* Lowercasing
* Stopword removal
* Punctuation removal
* Digit removal (optional)
* Language detection (optional)

üìå Used in **almost every NLP project**

---

#### C. Advanced Text Preprocessing

Used in complex NLP systems (chatbots, QA systems).

**Techniques:**

* POS Tagging (Part of Speech)
* Parsing (sentence structure)
* Coreference Resolution (he, she, it ‚Üí who?)

---

## 6. Step 3: Feature Engineering

### Meaning

Convert **text into numbers**.

üìå ML & DL models work **only on numbers**

---

### Simple example

For each review:

* Count positive words
* Count negative words
* Count neutral words

**Text ‚Üí Numbers ‚Üí Model**

---

### Common Feature Engineering Techniques

* Bag of Words (BoW)
* TF-IDF
* Word Embeddings
* Deep Learning embeddings

---

### ML vs Deep Learning (Important)

#### Machine Learning

* You manually create features
* Needs domain knowledge
* Results are interpretable

‚ùå **Disadvantage:** More effort

---

#### Deep Learning

* Features are learned automatically
* Needs large data
* Works like a black box

‚ùå **Disadvantage:** Less interpretability

---

## 7. Step 4: Modeling & Evaluation

### Modeling

Apply algorithms on features.

**Approaches:**

* Rule-based (very less data)
* Machine Learning algorithms
* Deep Learning models
* Cloud APIs (ready-made solutions)

**Choice depends on:**

* Amount of data
* Nature of problem

---

### Evaluation (Very important)

#### Intrinsic Evaluation (Technical)

* Accuracy
* Precision
* Recall
* Confusion Matrix
* Perplexity (for text generation)

---

#### Extrinsic Evaluation (Business)

* User engagement
* Click rate
* Product usage

üìå Good accuracy ‚â† good product
üìå Business metrics matter

---

## 8. Step 5: Deployment

Making the model usable by users.

### Deployment stages

#### Deployment

* API / Microservice
* App / Website / Chatbot

#### Monitoring

* Track model performance
* Dashboards
* Detect performance drop

#### Updating

* Retrain with new data
* Replace old models
* Online learning (optional)

---

## 9. Important Points to Remember

* NLP pipeline is **not linear**
* You may go back and forth between steps
* Pipeline differs for ML and Deep Learning
* Real-world projects are **iterative**

---

## 10. Assignment (Given in lecture)

### Problem

Detect **duplicate questions on Quora**.

### You must think about:

* Data source
* Text cleaning steps
* Feature engineering approach
* Algorithm selection
* Evaluation metrics
* Deployment strategy
* Monitoring & updates

üìå No coding required
üìå Focus on **thinking process**

---

## 11. Final Summary

* NLP is not just algorithms
* Pipeline thinking is critical
* **Data ‚Üí Clean ‚Üí Features ‚Üí Model ‚Üí Deploy**
* This approach helps in **real industry projects**

---
# üìò Text Preprocessing in NLP ‚Äì Simple English Notes (Part 1)

---

## 1. Introduction: Where Text Preprocessing Fits

In an NLP Pipeline, the main steps are:

* Data Acquisition
* Text Preprocessing ‚úÖ (This video focuses on this)
* Feature Engineering
* Modeling
* Deployment

üëâ Once you collect raw text data, you cannot directly apply ML models.
üëâ You must clean and preprocess the text first.

### Types of Text Preprocessing

* **Basic Text Preprocessing** (covered here)
* **Advanced Text Processing** (POS tagging, parsing, coreference resolution ‚Äì future videos)

---

## 2. Why Text Preprocessing Is Important

* Raw text is dirty and inconsistent
* Same word may appear in different formats
* Noise increases model complexity

Cleaning text improves:

* Accuracy
* Speed
* Model understanding

‚ö†Ô∏è **Important Rule:**
You do NOT apply all steps to every dataset.
You choose steps based on the problem and data type.

---

## 3. Lowercasing

### What is Lowercasing?

Convert all characters in text to lowercase.

### Why?

* ‚ÄúBasic‚Äù and ‚Äúbasic‚Äù should be treated as the same word
* Reduces duplicate vocabulary
* Simplifies model learning

### Example

**Without lowercasing:**

Basic ‚â† basic

**With lowercasing:**

basic = basic

### Python Code (Single Text)

```python
text = "This Movie Is AMAZING"
text.lower()
```

### Python Code (Dataset ‚Äì Pandas)

```python
df['review'] = df['review'].str.lower()
```

üìå This is usually the first step in text preprocessing.

---

## 4. Removing HTML Tags

### Problem

When text is scraped from websites, it often contains HTML tags like:

* `<p>`, `<br>`, `<a>`, `<div>`

These tags:

* Help browsers
* Do NOT help ML models

### Solution: Remove HTML Tags

### Python Code (Using Regex)

```python
import re

def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return re.sub(pattern, '', text)
```

### Example

```python
text = "<p>This movie was <b>great</b></p>"
remove_html_tags(text)
```

‚úÖ **Output:**

```
This movie was great
```

### Apply to Dataset

```python
df['review'] = df['review'].apply(remove_html_tags)
```

---

## 5. Removing URLs

### Problem

Text from:

* Twitter
* WhatsApp
* Instagram
* Reviews

Often contains URLs like:

* [https://example.com](https://example.com)
* [www.site.com](http://www.site.com)

URLs:

* Add noise
* Do not contribute to sentiment or meaning

### Python Code

```python
def remove_urls(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return re.sub(pattern, '', text)
```

### Example

```python
text = "Check this https://google.com now!"
remove_urls(text)
```

‚úÖ **Output:**

```
Check this  now!
```

---

## 6. Removing Punctuation

### Why Remove Punctuation?

* Punctuation becomes separate tokens
* Increases vocabulary size unnecessarily
* Confuses ML models

### Example

Hello! ‚Üí "Hello" + "!"

### Method 1 (Slow ‚Äì NOT recommended for big data)

```python
import string

def remove_punctuation(text):
    for char in string.punctuation:
        text = text.replace(char, '')
    return text
```

‚ùå Very slow for large datasets

### Method 2 (FAST & Recommended)

```python
import string

def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
```

‚úÖ Much faster
‚úÖ Best for large datasets

### Apply to Dataset

```python
df['tweet'] = df['tweet'].apply(remove_punctuation)
```

---

## 7. Chat Word Treatment (Short Forms)

### Problem

In chat and social media, people use shortcuts:

| Short Form | Meaning              |
| ---------- | -------------------- |
| GN         | Good Night           |
| IMO        | In My Opinion        |
| IMHO       | In My Honest Opinion |
| BRB        | Be Right Back        |

ML models cannot understand shortcuts.

### Solution

Replace shortcuts using a dictionary

### Example Dictionary

```python
chat_words = {
    "gn": "good night",
    "imo": "in my opinion",
    "imho": "in my honest opinion",
    "brb": "be right back"
}
```

### Python Code

```python
def chat_word_conversion(text):
    words = text.split()
    converted_words = []

    for word in words:
        if word.lower() in chat_words:
            converted_words.append(chat_words[word.lower()])
        else:
            converted_words.append(word)

    return " ".join(converted_words)
```

### Example

```python
text = "IMHO this movie is great GN"
chat_word_conversion(text)
```

‚úÖ **Output:**

```
in my honest opinion this movie is great good night
```

---

## 8. Spelling Correction

### Problem

Misspellings create multiple versions of the same word

Example:

notebook ‚â† noteebok

This:

* Increases vocabulary
* Reduces model accuracy

### Using TextBlob (Simple & Effective)

```python
from textblob import TextBlob

text = "I luvv this moovie"
corrected_text = str(TextBlob(text).correct())
print(corrected_text)
```

‚úÖ **Output:**

```
I love this movie
```

‚ö†Ô∏è **Note:**

* Works well for common English
* For domain-specific data, custom spell checkers may be needed

---

## 9. Removing Stop Words

### What are Stop Words?

Common words that:

* Help sentence formation
* Do NOT add meaning

Examples:

is, am, are, the, a, an, in

### Why Remove?

* Reduce noise
* Improve feature quality

‚ö†Ô∏è **Do NOT remove stop words for:**

* POS tagging
* Grammar analysis

### Using NLTK

```python
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
```

### Python Code

```python
def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)
```

### Example

```python
text = "this is a very good movie"
remove_stopwords(text)
```

‚úÖ **Output:**

```
good movie
```

---

## 10. Handling Emojis

### Problem

ML models cannot understand emojis directly

### Two Options

* Remove emojis ‚ùå
* Replace emojis with meaning ‚úÖ (Better)

### Option 1: Remove Emojis

```python
import re

def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)
```

### Option 2: Replace Emojis with Meaning (Recommended)

```python
import emoji

def replace_emojis(text):
    return emoji.demojize(text)
```

### Example

```python
text = "I love this movie üòçüî•"
replace_emojis(text)
```

‚úÖ **Output:**

```
I love this movie :smiling_face_with_heart_eyes: :fire:
```

---

## üîë Key Takeaways

* Text preprocessing is mandatory
* Choose steps based on data
* For large datasets ‚Üí always prefer efficient methods
* Clean text = Better ML models

  # üìò Text Preprocessing ‚Äì Part 2

## Tokenization, Stemming & Lemmatization (Simple English Notes)

---

## 1. Tokenization (MOST IMPORTANT STEP)

### What is Tokenization?

Tokenization is the process of breaking text into smaller units called **tokens**.

Tokens can be:

* Words
* Sentences
* Subwords (advanced)

### Example

**Sentence:**

```
I am an Indian
```

**Word Tokens:**

```
["I", "am", "an", "Indian"]
```

**Sentence Tokens:**

```
["I am an Indian.", "I love my country."]
```

---

## 2. Why Tokenization is Important?

* Machine Learning models do not understand raw text
* They work only with numbers

Before converting text into numbers:

* We must split text properly

‚ùå Wrong tokenization ‚Üí wrong features ‚Üí bad model performance

### Real Example

**Sentence:**

```
I am new in New Delhi
```

If tokenization is wrong:

```
new ‚â† New
```

Correct tokenization + lowercasing:

```
["i", "am", "new", "in", "new", "delhi"]
```

‚úÖ Unique words = 4 (not 5)

üëâ Wrong tokenization confuses the model

---

## 3. Types of Tokenization

### 1Ô∏è‚É£ Word Tokenization

* Split sentence into words

### 2Ô∏è‚É£ Sentence Tokenization

* Split paragraph into sentences

üìå Choice depends on:

* Your project
* Feature engineering logic

üìå Most NLP tasks use **word tokenization**

---

## 4. Problems in Tokenization (Real-World Challenges)

Tokenization is **NOT** as simple as splitting by space.

### Common Issues:

* **Punctuation** ‚Üí `Delhi!`
* **Numbers + units** ‚Üí `5km ‚Üí 5 + km`
* **Email IDs** ‚Üí `help@gmail.com`
* **Abbreviations** ‚Üí `U.S., Ph.D.`
* **Hyphenated words** ‚Üí `well-known`

üëâ Simple `.split()` fails in these cases

---

## 5. Tokenization Techniques

### üîπ Technique 1: Python `split()` (VERY BASIC)

```python
text = "I am going to Delhi!"
tokens = text.split()
print(tokens)
```

‚ùå Output:

```
['I', 'am', 'going', 'to', 'Delhi!']
```

‚ùå Problem:

```
Delhi! ‚â† Delhi
```

üìå Use only for very clean text

---

### üîπ Technique 2: Regular Expressions (Better but Complex)

```python
import re

text = "I am going to Delhi!"
tokens = re.findall(r'\w+', text)
print(tokens)
```

‚úÖ Output:

```
['I', 'am', 'going', 'to', 'Delhi']
```

‚ö†Ô∏è Still has problems with:

* Emails
* Units (5km)
* Abbreviations

---

### üîπ Technique 3: NLTK Tokenizer (GOOD)

#### Word Tokenization

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

text = "I am going to Delhi!"
tokens = word_tokenize(text)
print(tokens)
```

‚úÖ Output:

```
['I', 'am', 'going', 'to', 'Delhi', '!']
```

#### Sentence Tokenization

```python
from nltk.tokenize import sent_tokenize

text = "I love India. Delhi is my city!"
sentences = sent_tokenize(text)
print(sentences)
```

‚úÖ Output:

```
['I love India.', 'Delhi is my city!']
```

‚ö†Ô∏è NLTK is good, but not perfect

---

### üîπ Technique 4: spaCy Tokenizer (BEST ‚≠ê)

üìå Industry-recommended tokenizer

Handles:

* Emails
* Numbers + units
* Abbreviations
* Punctuation
* Complex grammar

#### Installation

```bash
pip install spacy
python -m spacy download en_core_web_sm
```

#### Word Tokenization using spaCy

```python
import spacy

nlp = spacy.load("en_core_web_sm")

text = "Email us at help@gmail.com or walk 5km today!"
doc = nlp(text)

tokens = [token.text for token in doc]
print(tokens)
```

‚úÖ Output:

```
['Email', 'us', 'at', 'help@gmail.com', 'or', 'walk', '5', 'km', 'today', '!']
```

üìå spaCy is the best choice for real projects

---

## 6. Key Takeaway on Tokenization

* Tokenization is NOT trivial
* Wrong tokens ‚Üí wrong features
* No tokenizer is 100% perfect

üìå Use:

* **spaCy** ‚Üí complex / real-world text
* **NLTK** ‚Üí simple tasks

---

## üå± Stemming

## 7. What is Stemming?

Stemming reduces words to their root form by removing prefixes/suffixes.

‚ö†Ô∏è The root word may NOT be a real English word.

### Example

| Word    | Stem  |
| ------- | ----- |
| working | work  |
| worked  | work  |
| studies | studi |
| movies  | movi  |

---

## 8. Why Use Stemming?

* Reduces vocabulary size
* Groups similar words
* Useful in Information Retrieval (Search Engines)

---

## 9. Stemming using NLTK (Porter Stemmer)

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

text = "working worked works"
tokens = word_tokenize(text)

stemmed_words = [stemmer.stem(word) for word in tokens]
print(stemmed_words)
```

‚úÖ Output:

```
['work', 'work', 'work']
```

‚ö†Ô∏è Problem:

```
studies ‚Üí studi
```

---

## üçÉ Lemmatization

## 10. What is Lemmatization?

Lemmatization converts words to their dictionary (meaningful) base form.

‚úÖ Output is always a real English word

---

## 11. Difference: Stemming vs Lemmatization

| Feature  | Stemming       | Lemmatization    |
| -------- | -------------- | ---------------- |
| Speed    | Fast           | Slow             |
| Output   | Not real words | Real words       |
| Logic    | Rule-based     | Dictionary-based |
| Accuracy | Lower          | Higher           |

---

## 12. Lemmatization using NLTK (WordNet)

```python
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

text = "movies are running better"
tokens = word_tokenize(text)

lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]
print(lemmatized_words)
```

‚úÖ Output:

```
['movie', 'are', 'running', 'better']
```

üìå Better results when POS tag is provided

---

## 13. When to Use What?

### Use **Stemming** when:

* Speed matters
* Output not shown to user
* Search engines, IR systems

### Use **Lemmatization** when:

* Output shown to user
* Grammar matters
* Chatbots, summaries, NLP apps

---

## 14. Final Summary

‚úÖ Tokenization is the foundation of NLP

‚úÖ spaCy gives the best tokenization

‚úÖ Stemming is fast but rough

‚úÖ Lemmatization is slow but accurate

‚úÖ Choose preprocessing steps based on use-case

---

## üìù Assignment (From Video)

Create your **own dataset** (do NOT use ready datasets):

**Movie Dataset Columns:**

* Movie Name
* Description (text)
* Genre (label)

Apply the following steps:

* Lowercasing
* Remove HTML
* Remove URLs
* Remove punctuation
* Stopwords removal
* Tokenization
* Stemming / Lemmatization

üìå Learning matters more than using ready datasets





