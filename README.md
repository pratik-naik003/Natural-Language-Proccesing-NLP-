# ğŸ“˜ Introduction to Natural Language Processing (NLP)

## Simple English Notes

---

## 1ï¸âƒ£ What is NLP?

**Natural Language Processing (NLP)** is a field of study that helps computers **understand, process, and generate human language**.

It is a combination of:

* **Linguistics** â†’ understanding language rules
* **Computer Science** â†’ programming & algorithms
* **Artificial Intelligence (AI)** â†’ making machines intelligent

### ğŸ”‘ Main Goal of NLP

To make machines:

* Understand what humans say or write
* Respond back in **natural human language**

NLP is not only about understanding text, but also about **speaking, replying, and interacting naturally**.

---

## 2ï¸âƒ£ Why is NLP Important? (Need of NLP)

Humans became powerful mainly because of:

* Communication
* Language

We shared ideas, taught future generations, and improved over time.

### ğŸš€ Next Big Revolution

The next big step in human progress is:

> **Talking to machines the same way we talk to humans**

### Evolution of Interaction

* **Earlier**:

  * Machines â†’ buttons, switches
  * Computers â†’ programming languages
  * Smartphones â†’ touch

* **Now**:

  * Voice
  * Text
  * Conversation

### Examples

* Google Assistant
* Siri
* Alexa
* Chatbots

ğŸ‘‰ NLP makes machines **friendly, usable, and intelligent** for everyone.

---

## 3ï¸âƒ£ Real-World Applications of NLP

### ğŸŸ¢ Everyday Applications

* Chatbots (customer support)
* Voice assistants
* Google Search smart answers
* Auto-reply emails
* Spam email detection

### ğŸŸ¢ Business & Industry

* Targeted advertisements
* Product review analysis
* Customer feedback analysis

### ğŸŸ¢ Social Media

* Hate-speech detection
* Adult content filtering
* Sentiment analysis on tweets
* Trend & opinion mining

### ğŸŸ¢ Search Engines

* Direct answers (no need to open websites)
* Knowledge graphs
* Smart suggestions

---

## 4ï¸âƒ£ Common NLP Tasks (Very Important)

If you want to become an **NLP Engineer**, these tasks are core skills:

### 1ï¸âƒ£ Text / Document Classification

Classifying text into categories:

* Sports
* Politics
* Technology
* Spam / Not spam

---

### 2ï¸âƒ£ Sentiment Analysis

Finding emotions from text:

* Positive
* Negative
* Neutral

Used in:

* Product reviews
* Movie reviews
* Social media

---

### 3ï¸âƒ£ Information Extraction

Extracting useful information:

* Names
* Locations
* Dates
* Product names

---

### 4ï¸âƒ£ Parts of Speech (POS) Tagging

Assigning labels to words:

* Noun
* Verb
* Adjective

Helps machines understand **sentence structure**.

---

### 5ï¸âƒ£ Language Detection & Translation

* Detect language automatically
* Translate text (Google Translate)

---

### 6ï¸âƒ£ Speech to Text & Text to Speech

* Voice â†’ Text
* Text â†’ Voice

Used in:

* Voice assistants
* Accessibility tools

---

### 7ï¸âƒ£ Text Summarization

* Convert long text into short summary
* Example: News apps like **Inshorts**

---

### 8ï¸âƒ£ Topic Modeling

Finding main topics in large text data

**Example**:

* Cricket article â†’ IPL, players, matches

---

### 9ï¸âƒ£ Text Generation

* Predict next word
* Auto-complete sentences
* ChatGPT-like systems

---

### ğŸ”Ÿ Spell Checking & Grammar Correction

* Detect spelling mistakes
* Suggest corrections

---

## 5ï¸âƒ£ Approaches Used in NLP

There are **3 main approaches** used in NLP:

---

### ğŸ”¹ 1. Rule-Based / Heuristic Approach (Old)

* Uses manual rules

**Example**:

* Count positive words vs negative words

**Uses**:

* Regular Expressions
* Dictionaries (WordNet)

âœ… Fast & accurate
âŒ Not scalable

---

### ğŸ”¹ 2. Machine Learning Based Approach

* Data-driven
* Learns patterns automatically
* Text is converted into numbers (vectorization)

**Common algorithms**:

* Naive Bayes
* Logistic Regression
* SVM
* Hidden Markov Models (POS tagging)

âœ… Better than rules
âŒ Needs feature engineering

---

### ğŸ”¹ 3. Deep Learning Based Approach (Modern)

**Big advantages**:

* Understands word order (sequence)
* Automatically learns features

**Popular models**:

* RNN
* LSTM
* GRU
* Transformers

ğŸš€ **Transformers changed NLP completely**.

ğŸ‘‰ All modern models like **BERT, GPT, Gemini** use Transformers.

---

## 6ï¸âƒ£ Challenges in NLP (Very Important)

NLP is hard because **language is complex**.

### âš ï¸ Major Challenges

1ï¸âƒ£ **Ambiguity**

* One sentence â†’ multiple meanings

2ï¸âƒ£ **Context Dependency**

* Same word â†’ different meaning

3ï¸âƒ£ **Slang & Idioms**

* â€œPulling someoneâ€™s legâ€

4ï¸âƒ£ **Sarcasm**

* â€œGreat jobâ€ (could mean bad)

5ï¸âƒ£ **Spelling Mistakes**

* Humans understand, machines struggle

6ï¸âƒ£ **Creativity**

* Poems, metaphors, jokes

7ï¸âƒ£ **Multiple Languages**

* Thousands of languages
* Very little data for many languages

---

## 7ï¸âƒ£ Why NLP is Still Evolving

* Language changes constantly
* New slang appears
* New contexts arise
* New cultures & dialects exist

ğŸ‘‰ We are using **less than 5%** of NLPâ€™s full potential today.

---

## 8ï¸âƒ£ Final Takeaway

* NLP is **powerful but challenging**
* It enables **humanâ€“machine communication**
* Used everywhere in modern technology
* Future growth depends heavily on NLP

---
# ğŸ“˜ NLP Pipeline â€“ Simple English Notes

---

## 1. Why this lecture is important

* NLP is a **difficult topic**
* Before jumping to algorithms, you must understand **how to think**
* This lecture teaches **how to approach any ML / NLP problem**
* In real companies, you donâ€™t just apply models â€” you build **end-to-end systems**

---

## 2. What is an NLP Pipeline?

An **NLP Pipeline** is a **series of steps** followed to build a complete NLP software system.

ğŸ‘‰ You cannot directly apply ML algorithms
ğŸ‘‰ You must go **step by step**

---

## 3. NLP Pipeline â€“ 5 Main Steps

1. **Data Acquisition**
2. **Text Preparation (Preprocessing)**
3. **Feature Engineering**
4. **Modeling + Evaluation**
5. **Deployment**

---

## 4. Step 1: Data Acquisition

### Meaning

Collecting text data for your NLP task.

ğŸ“Œ Without data â†’ **NLP system is impossible**

---

### Data availability scenarios

#### Case 1: Data already available (Internal data)

**Examples:**

* CSV file
* Company database

**What to do:**

* If CSV â†’ directly use it
* If database â†’ talk to data engineering team

---

#### Case 2: Data available externally

**Sources:**

* Public datasets
* Web scraping (using BeautifulSoup)
* APIs (using `requests`)
* PDFs â†’ extract text
* Images â†’ OCR
* Audio â†’ Speech-to-Text

âš ï¸ **Challenges:**

* Noisy data
* Website structure changes
* Unwanted text

---

#### Case 3: No data available

**What to do:**

* Collect data manually (forms, feedback)
* Label data yourself
* Start with rule-based system
* Move to ML later when data increases

---

### Data Augmentation (when data is less)

Create **synthetic data** from existing data.

**Techniques:**

* Synonym replacement
* Word order change
* Back translation
* Adding small noise

**Purpose:**

* Increase dataset size
* Improve model performance

---

## 5. Step 2: Text Preparation (Text Preprocessing)

### Goal

Make text **clean and machine-readable**

---

### Three levels of preprocessing

#### A. Basic Cleaning

* Remove HTML tags
* Handle emojis (Unicode normalization)
* Fix spelling mistakes
* Remove unwanted symbols

---

#### B. Basic Text Preprocessing (Most important)

**Common steps:**

* Tokenization (sentence & word)
* Lowercasing
* Stopword removal
* Punctuation removal
* Digit removal (optional)
* Language detection (optional)

ğŸ“Œ Used in **almost every NLP project**

---

#### C. Advanced Text Preprocessing

Used in complex NLP systems (chatbots, QA systems).

**Techniques:**

* POS Tagging (Part of Speech)
* Parsing (sentence structure)
* Coreference Resolution (he, she, it â†’ who?)

---

## 6. Step 3: Feature Engineering

### Meaning

Convert **text into numbers**.

ğŸ“Œ ML & DL models work **only on numbers**

---

### Simple example

For each review:

* Count positive words
* Count negative words
* Count neutral words

**Text â†’ Numbers â†’ Model**

---

### Common Feature Engineering Techniques

* Bag of Words (BoW)
* TF-IDF
* Word Embeddings
* Deep Learning embeddings

---

### ML vs Deep Learning (Important)

#### Machine Learning

* You manually create features
* Needs domain knowledge
* Results are interpretable

âŒ **Disadvantage:** More effort

---

#### Deep Learning

* Features are learned automatically
* Needs large data
* Works like a black box

âŒ **Disadvantage:** Less interpretability

---

## 7. Step 4: Modeling & Evaluation

### Modeling

Apply algorithms on features.

**Approaches:**

* Rule-based (very less data)
* Machine Learning algorithms
* Deep Learning models
* Cloud APIs (ready-made solutions)

**Choice depends on:**

* Amount of data
* Nature of problem

---

### Evaluation (Very important)

#### Intrinsic Evaluation (Technical)

* Accuracy
* Precision
* Recall
* Confusion Matrix
* Perplexity (for text generation)

---

#### Extrinsic Evaluation (Business)

* User engagement
* Click rate
* Product usage

ğŸ“Œ Good accuracy â‰  good product
ğŸ“Œ Business metrics matter

---

## 8. Step 5: Deployment

Making the model usable by users.

### Deployment stages

#### Deployment

* API / Microservice
* App / Website / Chatbot

#### Monitoring

* Track model performance
* Dashboards
* Detect performance drop

#### Updating

* Retrain with new data
* Replace old models
* Online learning (optional)

---

## 9. Important Points to Remember

* NLP pipeline is **not linear**
* You may go back and forth between steps
* Pipeline differs for ML and Deep Learning
* Real-world projects are **iterative**

---

## 10. Assignment (Given in lecture)

### Problem

Detect **duplicate questions on Quora**.

### You must think about:

* Data source
* Text cleaning steps
* Feature engineering approach
* Algorithm selection
* Evaluation metrics
* Deployment strategy
* Monitoring & updates

ğŸ“Œ No coding required
ğŸ“Œ Focus on **thinking process**

---

## 11. Final Summary

* NLP is not just algorithms
* Pipeline thinking is critical
* **Data â†’ Clean â†’ Features â†’ Model â†’ Deploy**
* This approach helps in **real industry projects**

---
# ğŸ“˜ Text Preprocessing in NLP â€“ Simple English Notes (Part 1)

---

## 1. Introduction: Where Text Preprocessing Fits

In an NLP Pipeline, the main steps are:

* Data Acquisition
* Text Preprocessing âœ… (This video focuses on this)
* Feature Engineering
* Modeling
* Deployment

ğŸ‘‰ Once you collect raw text data, you cannot directly apply ML models.
ğŸ‘‰ You must clean and preprocess the text first.

### Types of Text Preprocessing

* **Basic Text Preprocessing** (covered here)
* **Advanced Text Processing** (POS tagging, parsing, coreference resolution â€“ future videos)

---

## 2. Why Text Preprocessing Is Important

* Raw text is dirty and inconsistent
* Same word may appear in different formats
* Noise increases model complexity

Cleaning text improves:

* Accuracy
* Speed
* Model understanding

âš ï¸ **Important Rule:**
You do NOT apply all steps to every dataset.
You choose steps based on the problem and data type.

---

## 3. Lowercasing

### What is Lowercasing?

Convert all characters in text to lowercase.

### Why?

* â€œBasicâ€ and â€œbasicâ€ should be treated as the same word
* Reduces duplicate vocabulary
* Simplifies model learning

### Example

**Without lowercasing:**

Basic â‰  basic

**With lowercasing:**

basic = basic

### Python Code (Single Text)

```python
text = "This Movie Is AMAZING"
text.lower()
```

### Python Code (Dataset â€“ Pandas)

```python
df['review'] = df['review'].str.lower()
```

ğŸ“Œ This is usually the first step in text preprocessing.

---

## 4. Removing HTML Tags

### Problem

When text is scraped from websites, it often contains HTML tags like:

* `<p>`, `<br>`, `<a>`, `<div>`

These tags:

* Help browsers
* Do NOT help ML models

### Solution: Remove HTML Tags

### Python Code (Using Regex)

```python
import re

def remove_html_tags(text):
    pattern = re.compile('<.*?>')
    return re.sub(pattern, '', text)
```

### Example

```python
text = "<p>This movie was <b>great</b></p>"
remove_html_tags(text)
```

âœ… **Output:**

```
This movie was great
```

### Apply to Dataset

```python
df['review'] = df['review'].apply(remove_html_tags)
```

---

## 5. Removing URLs

### Problem

Text from:

* Twitter
* WhatsApp
* Instagram
* Reviews

Often contains URLs like:

* [https://example.com](https://example.com)
* [www.site.com](http://www.site.com)

URLs:

* Add noise
* Do not contribute to sentiment or meaning

### Python Code

```python
def remove_urls(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return re.sub(pattern, '', text)
```

### Example

```python
text = "Check this https://google.com now!"
remove_urls(text)
```

âœ… **Output:**

```
Check this  now!
```

---

## 6. Removing Punctuation

### Why Remove Punctuation?

* Punctuation becomes separate tokens
* Increases vocabulary size unnecessarily
* Confuses ML models

### Example

Hello! â†’ "Hello" + "!"

### Method 1 (Slow â€“ NOT recommended for big data)

```python
import string

def remove_punctuation(text):
    for char in string.punctuation:
        text = text.replace(char, '')
    return text
```

âŒ Very slow for large datasets

### Method 2 (FAST & Recommended)

```python
import string

def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))
```

âœ… Much faster
âœ… Best for large datasets

### Apply to Dataset

```python
df['tweet'] = df['tweet'].apply(remove_punctuation)
```

---

## 7. Chat Word Treatment (Short Forms)

### Problem

In chat and social media, people use shortcuts:

| Short Form | Meaning              |
| ---------- | -------------------- |
| GN         | Good Night           |
| IMO        | In My Opinion        |
| IMHO       | In My Honest Opinion |
| BRB        | Be Right Back        |

ML models cannot understand shortcuts.

### Solution

Replace shortcuts using a dictionary

### Example Dictionary

```python
chat_words = {
    "gn": "good night",
    "imo": "in my opinion",
    "imho": "in my honest opinion",
    "brb": "be right back"
}
```

### Python Code

```python
def chat_word_conversion(text):
    words = text.split()
    converted_words = []

    for word in words:
        if word.lower() in chat_words:
            converted_words.append(chat_words[word.lower()])
        else:
            converted_words.append(word)

    return " ".join(converted_words)
```

### Example

```python
text = "IMHO this movie is great GN"
chat_word_conversion(text)
```

âœ… **Output:**

```
in my honest opinion this movie is great good night
```

---

## 8. Spelling Correction

### Problem

Misspellings create multiple versions of the same word

Example:

notebook â‰  noteebok

This:

* Increases vocabulary
* Reduces model accuracy

### Using TextBlob (Simple & Effective)

```python
from textblob import TextBlob

text = "I luvv this moovie"
corrected_text = str(TextBlob(text).correct())
print(corrected_text)
```

âœ… **Output:**

```
I love this movie
```

âš ï¸ **Note:**

* Works well for common English
* For domain-specific data, custom spell checkers may be needed

---

## 9. Removing Stop Words

### What are Stop Words?

Common words that:

* Help sentence formation
* Do NOT add meaning

Examples:

is, am, are, the, a, an, in

### Why Remove?

* Reduce noise
* Improve feature quality

âš ï¸ **Do NOT remove stop words for:**

* POS tagging
* Grammar analysis

### Using NLTK

```python
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
```

### Python Code

```python
def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)
```

### Example

```python
text = "this is a very good movie"
remove_stopwords(text)
```

âœ… **Output:**

```
good movie
```

---

## 10. Handling Emojis

### Problem

ML models cannot understand emojis directly

### Two Options

* Remove emojis âŒ
* Replace emojis with meaning âœ… (Better)

### Option 1: Remove Emojis

```python
import re

def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)
```

### Option 2: Replace Emojis with Meaning (Recommended)

```python
import emoji

def replace_emojis(text):
    return emoji.demojize(text)
```

### Example

```python
text = "I love this movie ğŸ˜ğŸ”¥"
replace_emojis(text)
```

âœ… **Output:**

```
I love this movie :smiling_face_with_heart_eyes: :fire:
```

---

## ğŸ”‘ Key Takeaways

* Text preprocessing is mandatory
* Choose steps based on data
* For large datasets â†’ always prefer efficient methods
* Clean text = Better ML models

  # ğŸ“˜ Text Preprocessing â€“ Part 2

## Tokenization, Stemming & Lemmatization (Simple English Notes)

---

## 1. Tokenization (MOST IMPORTANT STEP)

### What is Tokenization?

Tokenization is the process of breaking text into smaller units called **tokens**.

Tokens can be:

* Words
* Sentences
* Subwords (advanced)

### Example

**Sentence:**

```
I am an Indian
```

**Word Tokens:**

```
["I", "am", "an", "Indian"]
```

**Sentence Tokens:**

```
["I am an Indian.", "I love my country."]
```

---

## 2. Why Tokenization is Important?

* Machine Learning models do not understand raw text
* They work only with numbers

Before converting text into numbers:

* We must split text properly

âŒ Wrong tokenization â†’ wrong features â†’ bad model performance

### Real Example

**Sentence:**

```
I am new in New Delhi
```

If tokenization is wrong:

```
new â‰  New
```

Correct tokenization + lowercasing:

```
["i", "am", "new", "in", "new", "delhi"]
```

âœ… Unique words = 4 (not 5)

ğŸ‘‰ Wrong tokenization confuses the model

---

## 3. Types of Tokenization

### 1ï¸âƒ£ Word Tokenization

* Split sentence into words

### 2ï¸âƒ£ Sentence Tokenization

* Split paragraph into sentences

ğŸ“Œ Choice depends on:

* Your project
* Feature engineering logic

ğŸ“Œ Most NLP tasks use **word tokenization**

---

## 4. Problems in Tokenization (Real-World Challenges)

Tokenization is **NOT** as simple as splitting by space.

### Common Issues:

* **Punctuation** â†’ `Delhi!`
* **Numbers + units** â†’ `5km â†’ 5 + km`
* **Email IDs** â†’ `help@gmail.com`
* **Abbreviations** â†’ `U.S., Ph.D.`
* **Hyphenated words** â†’ `well-known`

ğŸ‘‰ Simple `.split()` fails in these cases

---

## 5. Tokenization Techniques

### ğŸ”¹ Technique 1: Python `split()` (VERY BASIC)

```python
text = "I am going to Delhi!"
tokens = text.split()
print(tokens)
```

âŒ Output:

```
['I', 'am', 'going', 'to', 'Delhi!']
```

âŒ Problem:

```
Delhi! â‰  Delhi
```

ğŸ“Œ Use only for very clean text

---

### ğŸ”¹ Technique 2: Regular Expressions (Better but Complex)

```python
import re

text = "I am going to Delhi!"
tokens = re.findall(r'\w+', text)
print(tokens)
```

âœ… Output:

```
['I', 'am', 'going', 'to', 'Delhi']
```

âš ï¸ Still has problems with:

* Emails
* Units (5km)
* Abbreviations

---

### ğŸ”¹ Technique 3: NLTK Tokenizer (GOOD)

#### Word Tokenization

```python
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

text = "I am going to Delhi!"
tokens = word_tokenize(text)
print(tokens)
```

âœ… Output:

```
['I', 'am', 'going', 'to', 'Delhi', '!']
```

#### Sentence Tokenization

```python
from nltk.tokenize import sent_tokenize

text = "I love India. Delhi is my city!"
sentences = sent_tokenize(text)
print(sentences)
```

âœ… Output:

```
['I love India.', 'Delhi is my city!']
```

âš ï¸ NLTK is good, but not perfect

---

### ğŸ”¹ Technique 4: spaCy Tokenizer (BEST â­)

ğŸ“Œ Industry-recommended tokenizer

Handles:

* Emails
* Numbers + units
* Abbreviations
* Punctuation
* Complex grammar

#### Installation

```bash
pip install spacy
python -m spacy download en_core_web_sm
```

#### Word Tokenization using spaCy

```python
import spacy

nlp = spacy.load("en_core_web_sm")

text = "Email us at help@gmail.com or walk 5km today!"
doc = nlp(text)

tokens = [token.text for token in doc]
print(tokens)
```

âœ… Output:

```
['Email', 'us', 'at', 'help@gmail.com', 'or', 'walk', '5', 'km', 'today', '!']
```

ğŸ“Œ spaCy is the best choice for real projects

---

## 6. Key Takeaway on Tokenization

* Tokenization is NOT trivial
* Wrong tokens â†’ wrong features
* No tokenizer is 100% perfect

ğŸ“Œ Use:

* **spaCy** â†’ complex / real-world text
* **NLTK** â†’ simple tasks

---

## ğŸŒ± Stemming

## 7. What is Stemming?

Stemming reduces words to their root form by removing prefixes/suffixes.

âš ï¸ The root word may NOT be a real English word.

### Example

| Word    | Stem  |
| ------- | ----- |
| working | work  |
| worked  | work  |
| studies | studi |
| movies  | movi  |

---

## 8. Why Use Stemming?

* Reduces vocabulary size
* Groups similar words
* Useful in Information Retrieval (Search Engines)

---

## 9. Stemming using NLTK (Porter Stemmer)

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

text = "working worked works"
tokens = word_tokenize(text)

stemmed_words = [stemmer.stem(word) for word in tokens]
print(stemmed_words)
```

âœ… Output:

```
['work', 'work', 'work']
```

âš ï¸ Problem:

```
studies â†’ studi
```

---

## ğŸƒ Lemmatization

## 10. What is Lemmatization?

Lemmatization converts words to their dictionary (meaningful) base form.

âœ… Output is always a real English word

---

## 11. Difference: Stemming vs Lemmatization

| Feature  | Stemming       | Lemmatization    |
| -------- | -------------- | ---------------- |
| Speed    | Fast           | Slow             |
| Output   | Not real words | Real words       |
| Logic    | Rule-based     | Dictionary-based |
| Accuracy | Lower          | Higher           |

---

## 12. Lemmatization using NLTK (WordNet)

```python
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

text = "movies are running better"
tokens = word_tokenize(text)

lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]
print(lemmatized_words)
```

âœ… Output:

```
['movie', 'are', 'running', 'better']
```

ğŸ“Œ Better results when POS tag is provided

---

## 13. When to Use What?

### Use **Stemming** when:

* Speed matters
* Output not shown to user
* Search engines, IR systems

### Use **Lemmatization** when:

* Output shown to user
* Grammar matters
* Chatbots, summaries, NLP apps

---

## 14. Final Summary

âœ… Tokenization is the foundation of NLP

âœ… spaCy gives the best tokenization

âœ… Stemming is fast but rough

âœ… Lemmatization is slow but accurate

âœ… Choose preprocessing steps based on use-case

---

## ğŸ“ Assignment (From Video)

Create your **own dataset** (do NOT use ready datasets):

**Movie Dataset Columns:**

* Movie Name
* Description (text)
* Genre (label)

Apply the following steps:

* Lowercasing
* Remove HTML
* Remove URLs
* Remove punctuation
* Stopwords removal
* Tokenization
* Stemming / Lemmatization

ğŸ“Œ Learning matters more than using ready datasets

# ğŸ“˜ Text Representation / Feature Extraction from Text

*(Simple English + Deep Explanation Notes â€“ Part 1)*

---

## 1ï¸âƒ£ Where This Topic Fits in NLP Pipeline

Recall the NLP Pipeline:

1. Data Collection
2. Text Preprocessing
3. **Text Representation / Feature Extraction âœ… (THIS VIDEO)**
4. Modeling
5. Evaluation
6. Deployment

ğŸ‘‰ After preprocessing, text must be converted into numbers.
ğŸ‘‰ Machine Learning algorithms do **not** understand text; they understand **numbers only**.

---

## 2ï¸âƒ£ What is Feature Extraction from Text?

### Simple Definition

Feature Extraction from Text means:

> Converting raw text into numerical features so that machine learning algorithms can process it.

It is also called:

* Text Representation
* Text Vectorization

### Why â€œVectorizationâ€?

Because text is finally converted into **vectors (arrays of numbers)**.

### Example Problem â€“ Sentiment Analysis

Text:

> "This movie is very good"

Output:

* Positive / Negative

âš ï¸ ML models do **not** understand English words.
They understand **math and numbers**.

So the flow is:

> Text â†’ Numbers â†’ ML Model â†’ Output

---

## 3ï¸âƒ£ Why Do We Need Feature Extraction?

### Famous ML Rule

> **Garbage In â†’ Garbage Out**

* Bad features â†’ Bad output
* Good features â†’ Good output

Even:

> A simple algorithm + good features
> can outperform
> a complex algorithm + bad features

### Text Example (Sentiment Analysis)

Possible numerical features:

* Number of positive words
* Number of negative words
* Length of sentence
* Average word length

ğŸ‘‰ These act as **numeric signals** for ML models.

---

## 4ï¸âƒ£ Why is Text â†’ Numbers Difficult?

### Compare with Other Data Types

#### ğŸ–¼ï¸ Image Data

* Image = Pixels
* Pixels = Numbers
  âœ” Easy to convert

#### ğŸ”Š Audio Data

* Audio = Waveform
* Waveform = Amplitude values
  âœ” Easy to convert

#### ğŸ“ Text Data

Example:

> "Hello, how are you?"

Problems:

* âŒ No direct numeric form
* âŒ Meaning is abstract
* âŒ Order, context, emotion matter

ğŸ‘‰ Thatâ€™s why **text representation is hard**.

---

## 5ï¸âƒ£ Core Idea Behind Text Representation (VERY IMPORTANT)

While converting text to numbers,
**semantic meaning must be preserved**.

If numbers do not capture meaning:
â†’ Model performance will be poor.

Success depends on:

* How well meaning is encoded
* Similar texts getting similar vectors
* Different texts getting distant vectors

---

## 6ï¸âƒ£ Popular Text Representation Techniques

### Techniques Overview

* One Hot Encoding
* Bag of Words (BoW)
* N-grams
* TF-IDF
* Custom Hand-crafted Features
* Word Embeddings (Word2Vec, GloVe) *(Next video)*
* Deep Learning embeddings *(Later)*

ğŸ‘‰ This video covers:

* One Hot Encoding
* Bag of Words

---

## 7ï¸âƒ£ Important Terminology (Very Important)

### Corpus

All text data combined together.

### Vocabulary

Set of **unique words** in the corpus.

### Document

One individual text (review, sentence, email).

### Word / Token

Single unit inside a document.

### Example (IMDB Dataset)

* 50,000 reviews â†’ 50,000 documents
* Each review = one document
* All words together = corpus
* Unique words = vocabulary

---

## 8ï¸âƒ£ One Hot Encoding (First Technique)

### Idea

Each word is represented as a **binary vector**.

### Example Dataset

| Document | Sentence             |
| -------- | -------------------- |
| D1       | people watch campus  |
| D2       | watch campus campus  |
| D3       | people write comment |
| D4       | campus write comment |

### Step 1: Build Vocabulary

Vocabulary = [people, watch, campus, write, comment]

Size = 5

### Step 2: One Hot Representation

| Word    | Vector      |
| ------- | ----------- |
| people  | [1,0,0,0,0] |
| watch   | [0,1,0,0,0] |
| campus  | [0,0,1,0,0] |
| write   | [0,0,0,1,0] |
| comment | [0,0,0,0,1] |

### Document Representation

A document is represented by combining vectors of its words.

---

## 9ï¸âƒ£ Problems with One Hot Encoding âŒ

### âŒ 1. Sparse Vectors

* Vocabulary size = 50,000
* Vector length = 50,000
* Only one `1`, rest are `0`

ğŸ‘‰ High memory and computation cost

### âŒ 2. Not Fixed Input Size

* Different sentences â†’ different shapes
* ML models need **fixed-size input**

### âŒ 3. Out-of-Vocabulary (OOV)

* New word at prediction time â†’ model fails

### âŒ 4. No Semantic Meaning

Example words:

* run, walk, bottle

But:

* run & walk are similar
* bottle is different

One-hot treats all equally âŒ

### Final Verdict

* âŒ Not used in real applications
* âœ” Used only to understand basics

---

## 1ï¸âƒ£0ï¸âƒ£ Bag of Words (BoW) â€“ MOST IMPORTANT

### Core Idea

Instead of asking:

> Is the word present?

Ask:

> How many times does the word appear?

### Vocabulary (Same as Before)

[people, watch, campus, write, comment]

### BoW Representation

| Document | people | watch | campus | write | comment |
| -------- | ------ | ----- | ------ | ----- | ------- |
| D1       | 1      | 1     | 1      | 0     | 0       |
| D2       | 0      | 1     | 2      | 0     | 0       |
| D3       | 1      | 0     | 0      | 1     | 1       |
| D4       | 0      | 0     | 1      | 1     | 1       |

âœ” Fixed size vectors
âœ” Works for new sentences
âœ” Widely used in text classification

---

## 1ï¸âƒ£1ï¸âƒ£ Intuition Behind Bag of Words

Documents with:

* Similar words
* Similar frequencies

â†’ have similar meaning

### Vector Space View

* Each document = a point in high-dimensional space
* Similarity measured using:

  * Cosine similarity
  * Distance metrics

---

## 1ï¸âƒ£2ï¸âƒ£ Bag of Words Implementation (sklearn)

```python
from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "people watch campus",
    "watch campus campus",
    "people write comment",
    "campus write comment"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

print(vectorizer.vocabulary_)
print(X.toarray())
```

### Output

**Vocabulary:**

```
{'people':0, 'watch':1, 'campus':2, 'write':3, 'comment':4}
```

**BoW Matrix:**

```
[[1 1 1 0 0]
 [0 1 2 0 0]
 [1 0 0 1 1]
 [0 0 1 1 1]]
```

---

## 1ï¸âƒ£3ï¸âƒ£ Important Parameters of CountVectorizer

* `lowercase=True`

  * Converts all text to lowercase

* `stop_words='english'`

  * Removes common words like: is, the, and, of

* `binary=True`

  * Uses only presence (1) or absence (0)
  * Useful in sentiment analysis

* `max_features`

  * Keeps only top-N frequent words

  ```python
  CountVectorizer(max_features=1000)
  ```

---

## 1ï¸âƒ£4ï¸âƒ£ Advantages of Bag of Words âœ…

* âœ” Simple and intuitive
* âœ” Fixed-size input
* âœ” Handles unseen sentences
* âœ” Works well for text classification
* âœ” Better than One Hot Encoding

---

## 1ï¸âƒ£5ï¸âƒ£ Disadvantages of Bag of Words âŒ

### âŒ 1. Sparse Vectors

Large vocabulary â†’ many zeros

### âŒ 2. Order Ignored

"movie is good"
"good is movie"

Same vector âŒ

### âŒ 3. Context Ignored

"This is good movie"
"This is not good movie"

Almost same vector âŒ

### âŒ 4. Weak Semantic Understanding

Meaning is only partially captured

---

## 1ï¸âƒ£6ï¸âƒ£ Why We Need Next Technique (N-grams)

Problems with BoW:

* Word order ignored
* Context loss
* Negation not handled properly

ğŸ‘‰ Next technique: **N-grams**

---

## âœ… Summary of Part-1

| Technique        | Used in Practice |
| ---------------- | ---------------- |
| One Hot Encoding | âŒ No             |
| Bag of Words     | âœ… Yes            |
| N-grams          | âœ… Yes            |
| TF-IDF           | âœ… Yes            |
| Word Embeddings  | âœ… Yes            |

---

ğŸ“Œ **End of Part-1: Text Representation Basics**






