{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8897cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68687d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd44664",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# punctuation list\n",
    "punctuations = string.punctuation\n",
    "# tokenize sentence\n",
    "sentence_words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5212f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation safely\n",
    "sentence_words = [word for word in sentence_words if word not in punctuations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e751ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Lemma\n",
      "-----------------------------------\n",
      "He                   He\n",
      "was                  wa\n",
      "running              running\n",
      "and                  and\n",
      "eating               eating\n",
      "at                   at\n",
      "same                 same\n",
      "time                 time\n",
      "He                   He\n",
      "has                  ha\n",
      "bad                  bad\n",
      "habit                habit\n",
      "of                   of\n",
      "swimming             swimming\n",
      "after                after\n",
      "playing              playing\n",
      "long                 long\n",
      "hours                hour\n",
      "in                   in\n",
      "the                  the\n",
      "Sun                  Sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ---------------------------\n",
    "# Download required resources (run once)\n",
    "# ---------------------------\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# ---------------------------\n",
    "# Initialize lemmatizer\n",
    "# ---------------------------\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ---------------------------\n",
    "# Input sentence\n",
    "# ---------------------------\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# ---------------------------\n",
    "# Tokenization\n",
    "# ---------------------------\n",
    "sentence_words = word_tokenize(sentence)\n",
    "\n",
    "# ---------------------------\n",
    "# Remove punctuation\n",
    "# ---------------------------\n",
    "sentence_words = [word for word in sentence_words if word not in string.punctuation]\n",
    "\n",
    "# ---------------------------\n",
    "# Print header\n",
    "# ---------------------------\n",
    "print(\"{:20} {}\".format(\"Word\", \"Lemma\"))\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# ---------------------------\n",
    "# Lemmatization (verb form)\n",
    "# ---------------------------\n",
    "for word in sentence_words:\n",
    "    print(\"{:20} {}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='n')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
